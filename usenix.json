[{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final6.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">One-size-fits-all solutions have not worked well in storage systems. This is true in the enterprise where noSQL, Map-Reduce and column-stores have added value to traditional database workloads. This is also true outside the enterprise. A recent paper [7] illustrated that even the single-desktop store is a rich mixture of file systems, databases and key-value stores. Yet, in research one-size-fits-all solutions are always tempting and point-optimizations emerge, with the current <em>theme du jour</em> being key-value stores [8].</p> <p class=\"p1\">Workloads naturally change their requirements over time (e.g., from update-intensive to query-intensive). This paper proposes research around a <em>multi-structured</em> storage architecture. Such architecture is composed of many lightweight data structures such as BTrees, key- value stores, graph stores and chunk stores. The call for modular storage and systems is not dissimilar to the Exokernel [4] or Anvil [10] approaches. The key difference that this paper argues about is that we want these data structures to co-exist in the same system. The system should then automatically use the right one at the right workload phase. To enable this technically, we propose to leverage the existing <em>N</em>-way redundancy in the data center and have each of <em>N</em> replicas embody a different data structure.</p><p>\u00a0</p></div>"], "author": ["Eno Thereska, Phil Gosset,\u00a0and Richard Harper, "], "title": ["Multi-structured Redundancy"], "affiliation": ["Microsoft Research, Cambridge, UK"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final11.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Data analytics and enterprise applications have very different storage functionality requirements. For this reason, enterprise deployments of data analytics are on a separate storage silo. This may generate additional costs and inefficiencies in data management, e.g., whenever data needs to be archived, copied, or migrated across silos. We introduce MixApart, a scalable data processing framework for shared enterprise storage systems. With MixApart, a single consolidated storage back-end manages enterprise data and services all types of workloads, thereby lowering hardware costs and simplifying data management. In addition, MixApart enables the local storage performance required by analytics through an integrated data caching and scheduling solution. Our preliminary evaluation shows that MixApart can be 45% faster than the traditional <em>ingest-then-compute</em> workflow used in enterprise IT analytics, while requiring one third of storage capacity when compared to HDFS.</p><p>\u00a0</p></div>"], "author": ["Madalin Mihailescu, ", "\u00a0Gokul Soundararajan, ", "\u00a0Cristiana Amza,\u00a0"], "title": ["MixApart: Decoupled Analytics for Shared Storage Systems"], "affiliation": ["University of Toronto;", "NetApp;", "University of Toronto"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final40.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Storage infrastructure in large-scale cloud data center environments must support applications with diverse, time-varying data access patterns while observing the quality of service. Deeper storage hierarchies induced by solid state and rotating media are enabling new storage management tradeoffs that do not apply uniformly to all application phases at all times. To meet service level requirements in such heterogeneous application phases, storage management needs to be <em>phase-aware</em> and <em>adaptive</em>, i.e., to identify specific storage access patterns of applications as they occur and customize their handling accordingly.</p> <p class=\"p1\">This paper presents LoadIQ, a novel, versatile, adaptive, application phase detector for networked (file and block) storage systems. In a live deployment, LoadIQ analyzes traces and emits phase labels learnt on the fly by using Support Vector Machines(SVM), a state of the art classifier. Such labels could be used to generate alerts or to trigger phase-specific system tuning. Our results show that LoadIQ is able to identify workload phases (such as in TPC-DS) with accuracy &gt; 93%.</p><p>\u00a0</p></div>"], "author": ["Pankaj Pipada, Achintya Kundu, K. Gopinath, \u00a0and Chiranjib Bhattacharyya, ", " Sai Susarla and P. C.\u00a0Nagesh, "], "title": ["LoadIQ: Learning to Identify Workload Phases from a Live Storage Trace"], "affiliation": ["Indian Institute of Science;", "NetApp"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final57.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Disk contention is a fact of life in modern data centers, with multiple applications sharing the storage resources of a single physical machine. Log-structured storage designs are ideally suited for such high-contention settings, but historically have suffered from performance problems due to cleaning overheads. In this paper, we introduce Gecko, a novel design for storage arrays where a single log structure is distributed across a chain of drives, physically separating the tail of the log (where writes occur) from its body<span class=\"s1\">1</span>. This design provides the benefits of logging \u2013 fast, sequential writes for any number of contending applications \u2013 while eliminating the disruptive effect of log cleaning activity on application I/O.</p><p>\u00a0</p></div>"], "author": ["Ji Yong Shin, ", "\u00a0Mahesh Balakrishnan, ", "\u00a0Lakshmi Ganesh, ", "\u00a0Tudor Marian, ", "\u00a0Hakim Weatherspoon,\u00a0"], "title": ["Gecko: A Contention-Oblivious Design for Cloud Storage"], "affiliation": ["Cornell University;", "Microsoft Research;", "UT Austin;", "Google;", "Cornell University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final39.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">We present a set-associative page cache for scalable parallelism of IOPS in multicore systems. The design eliminates lock contention and hardware cache misses by partitioning the global cache into many independent page sets, each requiring a small amount of metadata that fits in few processor cache lines. We extend this design with message passing among processors in a non-uniform memory architecture (NUMA). We evaluate the set-associative cache on 12-core processors and a 48-core NUMA to show that it realizes the scalable IOPS of direct I/O (no caching) and matches the cache hits rates of Linux\u2019s page cache. Set-associative caching maintains IOPS at scale in contrast to Linux for which IOPS crash beyond eight parallel threads.</p><p>\u00a0</p></div>"], "author": ["Da Zheng, Randal Burns, and Alexander S. Szalay, "], "title": ["A Parallel Page Cache: IOPS and Caching for Multicore Systems"], "affiliation": ["Johns Hopkins University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final22.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Multi-tiered storage systems using tiers of SSD and traditional hard disk is one of the fastest growing trends in the storage industry. Although using multiple tiers provides a flexible trade-off in terms of IOPS performance and storage capacity, we believe that providing performance isolation and QoS guarantees among various clients, gets significantly more challenging in such environments. Existing solutions focus mainly on either disk-based or SSD-based storage backends. In particular, the notion of <em>IO cost</em> that is used by existing solutions gets very hard to estimate or use.</p> <p class=\"p1\">In this paper, we first argue that providing QoS in multi-tiered systems is quite challenging and existing solutions aren\u2019t good enough for such cases. To handle their drawbacks, we use a model of storage QoS called as <em>reward scheduling</em> and a corresponding algorithm, which favors the clients whose IOs are less costly on the back-end storage array for reasons such as better locality, read-mostly sequentiality, smaller working set as compared to SSD allocation etc. This allows for higher efficiency of the underlying system while providing desirable performance isolation. These results are validated using a simulation-based modeling of a multi-tiered storage system. We make a case that QoS in multi-tiered storage is an open problem and hope to encourage future research in this area.</p><p>\u00a0</p></div>"], "author": ["Ahmed Elnably and Hui Wang, ", " Ajay Gulati, ", " Peter Varman, "], "title": ["Ef\ufb01cient QoS for Multi-Tiered Storage Systems"], "affiliation": ["Rice University;", "VMware Inc.;", "Rice University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final18_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">In this work, we propose a new batching scheme called <em>temporal merge</em>, which dispatches discontiguous block requests using a single I/O operation. It overcomes the disadvantages of narrow block interface and enables an OS to exploit peak throughput of a storage device for small random requests as well as a single large request. Temporal merge significantly enhances device and channel utilization regardless of access sequentiality of a workload, which has not been achievable by traditional schemes.</p> <p class=\"p1\">We extended the block I/O interface of a DRAM-based SSD in cooperation with its vendor, and implemented temporal merge into I/O subsystem in Linux 2.6.32. The experimental results show that under multi-threaded random access workload, the proposed solution can achieve 87%\u223c100% of peak throughput of the SSD. We expect that the new temporal merge interface will lead to better design of future host controller interfaces such as NVMHCI for next-generation storage devices.</p><p>\u00a0</p></div>"], "author": ["Young Jin Yu, ", "\u00a0Dong In Shin, ", "\u00a0Woong Shin, Nae Young Song, Hyeonsang Eom, and Heon Young Yeom,\u00a0"], "title": ["Exploiting Peak Device Throughput from Random Access Workload"], "affiliation": ["Seoul National University;", "Taejin Infotec, Korea;", "Seoul National University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final48.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">This paper presents a detector of soon-to-fail disks based on a combination of statistical models. During operation the detector takes as input a performance signal from each disk and sends and alarm when there is enough evidence (according to the models) that the disk is not healthy. The parameters of these models are automatically trained using signals from healthy and failed disks. In an evaluation on a population of 1190 production disks from a popular customer-facing internet service, the detector was able to predict 15 out of the 17 failed disks (88.2% detection) with 30 false alarms (2.56% false positive rate).</p><p>\u00a0</p></div>"], "author": ["Moises Goldszmidt, "], "title": ["Finding Soon-to-Fail Disks in a Haystack"], "affiliation": ["Microsoft Research"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final55.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Exploiting internal parallelism over hundreds NAND flash memory is becoming a key design issue in high-speed Solid State Disks (SSDs). In this work, we simulated a cycle-accurate SSD platform with twenty four page allocation strategies, geared toward exploiting both system-level parallelism and flash-level parallelism with a variety of design parameters. Our extensive experimental analysis reveals that 1) the previously-proposed channel-and-way striping based page allocation scheme is not the best from a performance perspective, 2) As opposed to the current perception that system and flash-level concurrency mechanisms are largely orthogonal, flash-level parallelism are interfered by the system-level concurrency mechanism employed, and 3) With most of the current parallel data access methods, internal resources are significantly under-utilized. Finally, we present several optimization points to achieve maximum internal parallelism.</p><p>\u00a0</p></div>"], "author": ["Myoungsoo Jung and Mahmut Kandemir, "], "title": ["An Evaluation of Different Page Allocation Strategies on High-Speed SSDs"], "affiliation": ["The Pennsylvania State University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final38_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">For backup storage, increasing compression allows users to protect more data without increasing their costs or storage footprint. Though removing duplicate regions (deduplication) and traditional compression have become widespread, further compression is attainable. We demonstrate how to efficiently add delta compression to deduplicated storage to compress similar (non-duplicate) regions. A challenge when adding delta compression is the large number of data regions to be indexed. We observed that stream-informed locality is effective for delta compression, so an index for delta compression is unnecessary, and we built the first storage system prototype to combine delta compression and deduplication with this technology. Beyond demonstrating extra compression benefits between 1.4-3.5X, we also investigate throughput and data integrity challenges that arise.</p><p>\u00a0</p></div>"], "author": ["Philip Shilane, Grant Wallace, Mark Huang, and Windsor Hsu, "], "title": ["Delta Compressed and Deduplicated Storage Using Stream-Informed Locality"], "affiliation": ["EMC Corporation"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final56_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Most compression algorithms used in storage systems today are based on an increasingly outmoded sequential processing model. Systems wishing to decompress blocks out-of-order or in parallel must reset the compressor\u2019s state before each block, reducing adaptiveness and limiting compression ratios. To remedy this situation, we present Non-Linear Compression, a novel compression model enabling systems to impose an arbitrary partial order on inter-block dependencies. Mutually unordered blocks may be compressed and decompressed out-of-order or in parallel, and a compressor can adaptively compress each block based on all causally prior blocks. This graph structure captures the system\u2019s data dependencies explicitly and completely, enabling the compressor to adapt using long-lived state without the constraint of sequential processing. Preliminary experiences with a simple Huffman compressor suggest that non-linear compression fits a diverse set of storage applications.</p><p>\u00a0</p></div>"], "author": ["Michael F. Nowlan, Bryan Ford,\u00a0and Ramakrishna Gummadi, "], "title": ["Non-Linear Compression: Gzip Me Not!"], "affiliation": ["Yale University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final53.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">A \u201chome\u201d sharing environment consists of the data sharing relationships between family members, friends, and acquaintances. We argue that this environment, far from being simple, has sharing and trust relationships as complex as any general-purpose network.</p> <p class=\"p1\">Such environments need strong access control and privacy guarantees. We show that avoiding information leakage requires both to be integrated directly into (rather than layered on top of) replication protocols, and propose a system structure that meets these guarantees.</p><p>\u00a0</p></div>"], "author": ["Vassilios Lekakis, Yunus Basagalar, and Pete Keleher,\u00a0"], "title": ["Don\u2019t Trust Your Roommate, or, Access Control and Replication Protocols in \u201cHome\u201d Environments"], "affiliation": ["University of Maryland"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final63.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Video content is quite unique from its storage footprint perspective. In a video distribution environment, a master video file needs to be transcoded into different resolutions, bitrates, codecs and containers to enable distribution to a wide variety of devices and media players over different kinds of networks. Our experiments show that when 8 master videos are transcoded into most popular 376 formats (derived from 8 resolutions and 6 containers), transcoded versions occupy 8 times more storage than the master video. One major challenge with efficiently storing such content is that traditional de-duplication algorithms cannot detect significant duplication between any 2 versions. Transcoding on-the-fly is a technique in which a distribution copy is created only when requested by a user. This technique saves storage but at the expense of extra compute cost and latency resulting from transcoding after a user request is received. In this paper we develop cost metrics that allow us to compare storage vs. compute costs and suggest when a transcoding on-the-fly solution can be cost effective. We also analyze how such a solution can be deployed in a practical storage system using access pattern information or a variant of ski-rent [1] online algorithm when such information is not available.</p><p>\u00a0</p></div>"], "author": ["Atish Kathpal, Mandar Kulkarni, and Ajay Bakre, "], "title": ["Analyzing Compute vs. Storage Tradeoff for Video-aware Storage Efficiency"], "affiliation": ["NetApp Inc."]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hotstorage12/hotstorage12-final52.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">The TokuFS file system outperforms write-optimized file systems by an order of magnitude on microdata write workloads, and outperforms read-optimized file systems by an order of magnitude on read workloads. Microdata write workloads include creating and destroying many small files, performing small unaligned writes within large files, and updating metadata. TokuFS is implemented using Fractal Tree indexes, which are primarily used in databases. TokuFS employs block-level compression to reduce its disk usage.</p><p>\u00a0</p></div>"], "author": ["John Esmet, ", " Michael A. Bender, ", " Martin Farach-Colton, ", " Bradley C. Kuszmaul, "], "title": ["The TokuFS Streaming File System"], "affiliation": ["Tokutek & Rutgers;", "Tokutek & Stony Brook;", "Tokutek & Rutgers;", "Tokutek & MIT"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Flat Datacenter Storage (FDS) is a high-performance, fault-tolerant, large-scale, locality-oblivious blob store. Using a novel combination of full bisection bandwidth networks, data and metadata striping, and flow control, FDS multiplexes an application\u2019s large-scale I/O across the available throughput and latency budget of every disk in a cluster. FDS therefore makes many optimizations around data locality unnecessary. Disks also communicate with each other at their full bandwidth, making recovery from disk failures extremely fast. FDS is designed for datacenter scale, fully distributing metadata operations that might otherwise become a bottleneck.\u00a0</p>\r\n<p>FDS applications achieve single-process read and write performance of more than 2GB/s. We measure recovery of 92GB data lost to disk failure in 6.2 s and recovery from a total machine failure with 655GB of data in 33.7 s. Application performance is also high: we describe our FDS-based sort application which set the 2012 world record for disk-to-disk sorting.</p></div>"], "author": ["\u00a0Edmund B. Nightingale, Jeremy Elson, and Jinliang Fan, ", "\u00a0 Owen Hofmann, ", "\u00a0 Jon Howell and Yutaka Suzue, "], "title": ["Flat Datacenter Storage"], "affiliation": ["Microsoft Research;", "University of\u00a0Texas at Austin;", "Microsoft Research"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-167.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Large-scale graph-structured computation is central to tasks ranging from targeted advertising to natural language processing and has led to the development of several graph-parallel abstractions including Pregel and GraphLab. However, the natural graphs commonly found in the real-world have highly skewed power-law degree distributions, which challenge the assumptions made by these abstractions, limiting performance and scalability.</p>\r\n<p>In this paper, we characterize the challenges of computation on natural graphs in the context of existing graphparallel abstractions. We then introduce the PowerGraph abstraction which exploits the internal structure of graph programs to address these challenges. Leveraging the PowerGraph abstraction we introduce a new approach to distributed graph placement and representation that exploits the structure of power-law graphs. We provide a detailed analysis and experimental evaluation comparing PowerGraph to two popular graph-parallel systems. Finally, we describe three different implementation strategies for PowerGraph and discuss their relative merits with empirical evaluations on large-scale real-world problems demonstrating order of magnitude gains.</p></div>"], "author": ["Joseph E. Gonzalez, Yucheng Low, Haijie Gu, and Danny Bickson, ", "Carlos Guestrin, "], "title": ["PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs"], "affiliation": ["Carnegie Mellon University; ", "University of Washington"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-126.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><div class=\"column\">\r\n<p><span>Current systems for graph computation require a distributed computing cluster to handle very large real-world\r\nproblems, such as analysis on social networks or the web\r\ngraph. While distributed computational resources have become more accessible, developing distributed graph algorithms still remains challenging, especially to non-experts.\r\n</span></p>\r\n<p><span>In this work, we present GraphChi, a </span><span>disk-based system\r\n</span><span>for computing efficiently on graphs with billions of edges.\r\nBy using a well-known method to break large graphs into\r\nsmall parts, and a novel </span><span>parallel sliding windows </span><span>method,\r\nGraphChi is able to execute several advanced data mining,\r\ngraph mining, and machine learning algorithms on very\r\nlarge graphs, using just a single consumer-level computer.\r\nWe further extend GraphChi to support graphs that </span><span>evolve\r\nover time, </span><span>and demonstrate that, on a single computer,\r\nGraphChi can process over one hundred thousand graph\r\nupdates per second, while simultaneously performing computation. We show, through experiments and theoretical\r\nanalysis, that GraphChi performs well on both SSDs and\r\nrotational hard drives.\r\n</span></p>\r\n<p><span>By repeating experiments reported for existing distributed systems, we show that, with only fraction of the\r\nresources, GraphChi can solve the same problems in very\r\nreasonable time. Our work makes large-scale graph computation available to anyone with a modern PC.\u00a0</span></p>\r\n</div></div>"], "author": ["Aapo Kyrola and Guy Blelloch, ", " Carlos Guestrin, "], "title": ["GraphChi: Large-Scale Graph Computation on Just a PC"], "affiliation": ["Carnegie Mellon University;", "University of Washington"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-35.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Modern extensible web platforms like Facebook and Yammer depend on third-party software to offer a rich experience to their users. Unfortunately, users running a third-party \u201capp\u201d have little control over what it does with their private data. Today\u2019s platforms offer only ad-hoc constraints on app behavior, leaving users an unfortunate trade-off between convenience and privacy. A principled approach to code confinement could allow the integration of untrusted code while enforcing flexible, end-to-end policies on data access. This paper presents a new web framework, Hails, that adds mandatory access control and a declarative policy language to the familiar MVC architecture. We demonstrate the flexibility of Hails through GitStar.com, a code-hosting website that enforces robust privacy policies on user data even while allowing untrusted apps to deliver extended features to users.</p></div>"], "author": ["Daniel B. Giffin, Amit Levy, Deian Stefan, David Terei, David Mazi\u00e8res, and John C. Mitchell, ", "Alejandro Russo, ", "\u00a0"], "title": ["Hails: Protecting Data Privacy in Untrusted Web Applications"], "affiliation": ["Stanford University; ", "Chalmers University"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-100.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Modern systems keep long memories. As we show in this paper, an adversary who gains access to a Linux system, even one that implements secure deallocation, can recover the contents of applications\u2019 windows, audio buffers, and data remaining in device drivers\u2014long after the applications have terminated.<br>We design and implement Lacuna, a system that allows users to run programs in \u201cprivate sessions.\u201d After the session is over, all memories of its execution are erased. The key abstraction in Lacuna is an ephemeral channel, which allows the protected program to talk to peripheral devices while making it possible to delete the memories of this communication from the host. Lacuna can run unmodified applications that use graphics, sound, USB input devices, and the network, with only 20 percentage points of additional CPU utilization.</p></div>"], "author": ["Alan M. Dunn, Michael Z. Lee, Suman Jana, Sangman Kim, Mark Silberstein, Yuanzhong Xu, Vitaly Shmatikov, and Emmett Witchel, "], "title": ["Eternal Sunshine of the Spotless Machine: Protecting Privacy with Ephemeral Channels"], "affiliation": ["The University of Texas at Austin"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-203.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Mobile-device theft and loss have reached gigantic proportions. Despite these threats, today\u2019s mobile devices are saturated with sensitive information due to operating systems that never securely erase data and applications that hoard it on the vulnerable device for performance or convenience. This paper presents CleanOS, a new Android-based operating system that manages sensitive data rigorously and maintains a clean environment at all times. To do so, CleanOS leverages a key property of today\u2019s mobile applications\u2014the use of trusted, cloudbased services. Specifically, CleanOS identifies and tracks sensitive data in RAM and on stable storage, encrypts it with a key, and evicts that key to the cloud when the data is not in active use on the device. We call this process idle eviction of sensitive data. To implement CleanOS, we used the TaintDroid mobile taint-tracking system to identify sensitive data locations and instrumented Android\u2019s Dalvik interpreter to securely evict that data after a specified period of non-use. Our experimental results show that CleanOS limits sensitive-data exposure drastically while incurring acceptable overheads on mobile networks.</p></div>"], "author": ["Yang Tang, Phillip Ames, Sravan Bhamidipati, Ashish Bijlani, Roxana Geambasu, and Nikhil Sarda, "], "title": ["CleanOS: Limiting Mobile Data Exposure with Idle Eviction"], "affiliation": ["Columbia University"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-11.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>In this paper we introduce a runtime system to allow unmodified multi-threaded applications to use multiple machines. The system allows threads to migrate freely between machines depending on the workload. Our prototype, COMET (Code Offload by Migrating Execution Transparently), is a realization of this design built on top of the Dalvik Virtual Machine. COMET leverages the underlying memory model of our runtime to implement distributed shared memory (DSM) with as few interactions between machines as possible. Making use of a new VM-synchronization primitive, COMET imposes little restriction on when migration can occur. Additionally, enough information is maintained so one machine may resume computation after a network failure.</p>\r\n<p>We target our efforts towards augmenting smartphones or tablets with machines available in the network. We demonstrate the effectiveness of COMET on several real applications available on Google Play. These applications include image editors, turn-based games, a trip planner, and math tools. Utilizing a server-class machine, COMET can offer significant speed-ups on these real applications when run on a modern smartphone. With WiFi and 3G networks, we observe geometric mean speed-ups of 2.88X and 1.27X relative to the Dalvik interpreter across the set of applications with speed-ups as high as 15X on some applications.</p></div>"], "author": ["Mark S. Gordon, D. Anoushe Jamshidi, Scott Mahlke, and Z. Morley Mao, ", "Xu Chen, "], "title": ["COMET: Code Offload by Migrating Execution Transparently"], "affiliation": ["University of Michigan; ", "AT&T Labs\u2014Research"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-91.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>The mobile-app marketplace is highly competitive. To maintain and improve the quality of their apps, developers need data about how their app is performing in the wild. The asynchronous, multi-threaded nature of mobile apps makes tracing difficult. The difficulties are compounded by the resource limitations inherent in the mobile platform. To address this challenge, we develop AppInsight, a system that instruments mobileapp binaries to automatically identify the critical path in user transactions, across asynchronous-call boundaries. AppInsight is lightweight, it does not require any input from the developer, and it does not require any changes to the OS. We used AppInsight to instrument 30 marketplace apps, and carried out a field trial with 30 users for over 4 months. We report on the characteristics of the critical paths that AppInsight found in this data. We also give real-world examples of how AppInsight helped developers improve the quality of their app.</p></div>"], "author": ["Lenin Ravindranath, Jitendra Padhye, Sharad Agarwal, Ratul Mahajan, Ian Obermiller, and Shahin Shayandeh, "], "title": ["AppInsight: Mobile App Performance Monitoring in the Wild"], "affiliation": ["Microsoft Research"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-23.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>To minimize the amount of data-shuffling I/O that occurs between the pipeline stages of a distributed dataparallel program, its procedural code must be optimized with full awareness of the pipeline that it executes in. Unfortunately, neither pipeline optimizers nor traditional compilers examine both the pipeline and procedural code of a data-parallel program so programmers must either hand-optimize their program across pipeline stages or live with poor performance. To resolve this tension between performance and programmability, this paper describes PeriSCOPE, which automatically optimizes a data-parallel program\u2019s procedural code in the context of data flow that is reconstructed from the program\u2019s pipeline topology. Such optimizations eliminate unnecessary code and data, perform early data filtering, and calculate small derived values (e.g., predicates) earlier in the pipeline, so that less data\u2014sometimes much less data\u2014is transferred between pipeline stages. We describe how PeriSCOPE is implemented and evaluate its effectiveness on real production jobs.</p></div>"], "author": ["Zhenyu Guo, ", "Xuepeng Fan, ", "Rishan Chen, ", " Jiaxing Zhang, Hucheng Zhou, and Sean McDirmid, ", " Chang Liu, ", "Wei Lin and Jingren Zhou, ", "Lidong Zhou, "], "title": ["Spotting Code Optimizations in Data-Parallel Pipelines through PeriSCOPE"], "affiliation": ["Microsoft Research Asia; ", "Microsoft Research Asia and Huazhong University of Science and Technology; ", "Microsoft Research Asia and Peking University;", "Microsoft Research Asia;", "Microsoft Research Asia and Shanghai Jiao Tong University; ", "Microsoft Bing; ", "Microsoft Research Asia"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-40.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>We present MegaPipe, a new API for efficient, scalable network I/O for message-oriented workloads. The design of MegaPipe centers around the abstraction of a channel\u2014a per-core, bidirectional pipe between the kernel and user space, used to exchange both I/O requests and event notifications. On top of the channel abstraction, we introduce three key concepts of MegaPipe: partitioning, lightweight socket (lwsocket), and batching.</p>\r\n<p>We implement MegaPipe in Linux and adapt memcached and nginx. Our results show that, by embracing a clean-slate design approach, MegaPipe is able to exploit new opportunities for improved performance and ease of programmability. In microbenchmarks on an 8-core server with 64 B messages, MegaPipe outperforms baseline Linux between 29% (for long connections) and 582% (for short connections). MegaPipe improves the performance of a modified version of memcached between 15% and 320%. For a workload based on real-world HTTP traces, MegaPipe boosts the throughput of nginx by 75%.</p></div>"], "author": ["Sangjin Han and Scott Marshall, ", "Byung-Gon Chun, ", "Sylvia Ratnasamy, "], "title": ["MegaPipe: A New Programming Interface for Scalable Network I/O "], "affiliation": ["University of California, Berkeley; ", "Yahoo! Research; ", "University of California, Berkeley"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-164.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>In this paper, we study the problem of answering queries about private data that is spread across multiple different databases. For instance, a medical researcher may want to study a possible correlation between travel patterns and certain types of illnesses. The necessary information exists today \u2013 e.g., in airline reservation systems and hospital records\u2014but it is maintained by two separate companies who are prevented by law from sharing this information with each other, or with a third party. This separation prevents the processing of such queries, even if the final answer, e.g., a correlation coefficient, would be safe to release.</p>\r\n<p>We present DJoin, a system that can process such distributed queries and can give strong differential privacy guarantees on the result. DJoin can support many SQLstyle queries, including joins of databases maintained by different entities, as long as they can be expressed using DJoin\u2019s two novel primitives: BN-PSI-CA, a differentially private form of private set intersection cardinality, and DCR, a multi-party combination operator that can aggregate noised cardinalities without compounding the individual noise terms. Our experimental evaluation shows that DJoin can process realistic queries at practical timescales: simple queries on three databases with 15,000 rows each take between 1 and 7.5 hours.</p></div>"], "author": ["Arjun Narayan and\u00a0Andreas Haeberlen,\u00a0"], "title": ["DJoin: Differentially Private Join Queries over Distributed Databases"], "affiliation": ["University of Pennsylvania"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-88.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Integer errors have emerged as an important threat to systems security, because they allow exploits such as buffer overflow and privilege escalation. This paper presents KINT, a tool that uses scalable static analysis to detect integer errors in C programs. KINT generates constraints from source code and user annotations, and feeds them into a constraint solver for deciding whether an integer error can occur. KINT introduces a number of techniques to reduce the number of false error reports. KINT identified more than 100 integer errors in the Linux kernel, the lighttpd web server, and OpenSSH, which were confirmed and fixed by the developers. Based on the experience with KINT, the paper further proposes a new integer family with NaN semantics to help developers avoid integer errors in C programs.</p></div>"], "author": ["Xi Wang and Haogang Chen, ", "Zhihao Jia, ", " Nickolai Zeldovich and M. Frans Kaashoek, "], "title": ["Improving Integer Security for Systems with KINT"], "affiliation": ["MIT CSAIL; ", "Tsinghua University IIIS;", "MIT CSAIL"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-115.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Current anonymous communication systems make a trade-off between weak anonymity among many nodes, via onion routing, and strong anonymity among few nodes, via DC-nets. We develop novel techniques in Dissent, a practical group anonymity system, to increase by over two orders of magnitude the scalability of strong, traffic analysis resistant approaches. Dissent derives its scalability from a client/server architecture, in which many unreliable clients depend on a smaller and more robust, but administratively decentralized, set of servers. Clients trust only that at least one server in the set is honest, but need not know or choose which server to trust. Unlike the quadratic costs of prior peer-to-peer DC-nets schemes, Dissent\u2019s client/server design makes communication and processing costs linear in the number of clients, and hence in anonymity set size. Further, Dissent\u2019s servers can unilaterally ensure progress, even if clients respond slowly or disconnect at arbitrary times, ensuring robustness against client churn, tail latencies, and DoS attacks. On DeterLab, Dissent scales to 5,000 online participants with latencies as low as 600 milliseconds for 600-client groups. An anonymous Web browsing application also shows that Dissent\u2019s performance suffices for interactive communication within smaller local-area groups.</p></div>"], "author": ["David Isaac Wolinsky, Henry Corrigan-Gibbs, and Bryan Ford, ", " Aaron Johnson, "], "title": ["Dissent in Numbers: Making Strong Anonymity Scale"], "affiliation": ["Yale University;", "U.S. Naval Research Laboratory"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-147.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>POIROT is a system that, given a patch for a newly discovered security vulnerability in a web application, helps administrators detect past intrusions that exploited the vulnerability. POIROT records all requests to the server during normal operation, and given a patch, re-executes requests using both patched and unpatched software, and reports to the administrator any request that executes differently in the two cases. A key challenge with this approach is the cost of re-executing all requests, and POIROT introduces several techniques to reduce the time required to audit past requests, including filtering requests based on their control flow and memoization of intermediate results across different requests.</p>\r\n<p>A prototype of POIROT for PHP accurately detects attacks on older versions of MediaWiki and HotCRP, given subsequently released patches. POIROT\u2019s techniques allow it to audit past requests 12\u201351\u00d7 faster than the time it took to originally execute the same requests, for patches to code executed by every request, under a realistic MediaWiki workload.</p></div>"], "author": ["Taesoo Kim,\u00a0Ramesh Chandra, and\u00a0Nickolai Zeldovich,\u00a0"], "title": ["Efficient Patch-based Auditing for Web Application Vulnerabilities"], "affiliation": ["MIT CSAIL"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-183.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>When first written in 2000, TinyOS\u2019s users were a handful of academic computer science researchers. A decade later, TinyOS averages 25,000 downloads a year, is in many commercial products, and remains a platform used for a great deal of sensor network, low-power systems, and wireless research.</p>\r\n<p>We focus on how technical and social decisions influenced this success, sometimes in surprising ways. As TinyOS matured, it evolved language extensions to help experts write efficient, robust systems. These extensions revealed insights and novel programming abstractions for embedded software. Using these abstractions, experts could build increasingly complex systems more easily than with other operating systems, making TinyOS the dominant choice.</p>\r\n<p>This success, however, came at a long-term cost. System design decisions that seem good at first can have unforeseen and undesirable implications that play out over the span of years. Today, TinyOS is a stable, selfcontained ecosystem that is discouraging to new users. Other systems, such as Arduino and Contiki, by remaining more accessible, have emerged as better solutions for simpler embedded sensing applications.</p></div>"], "author": ["Philip Levis,\u00a0"], "title": ["Experiences from a Decade of TinyOS Development"], "affiliation": ["Stanford University"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-103.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Concurrency bugs are widespread in multithreaded programs. Fixing them is time-consuming and error-prone. We present CFix, a system that automates the repair of concurrency bugs. CFix works with a wide variety of concurrency-bug detectors. For each failure-inducing interleaving reported by a bug detector, CFix first determines a combination of mutual-exclusion and order relationships that, once enforced, can prevent the buggy interleaving. CFix then uses static analysis and testing to determine where to insert what synchronization operations to force the desired mutual-exclusion and order relationships, with a best effort to avoid deadlocks and excessive performance losses. CFix also simplifies its own patches by merging fixes for related bugs.</p>\r\n<p>Evaluation using four different types of bug detectors and thirteen real-world concurrency-bug cases shows that CFix can successfully patch these cases without causing deadlocks or excessive performance degradation. Patches automatically generated by CFix are of similar quality to those manually written by developers.</p></div>"], "author": ["Guoliang Jin, Wei Zhang, Dongdong Deng, Ben Liblit, and Shan Lu, "], "title": ["Automated Concurrency-Bug Fixing"], "affiliation": ["University of Wisconsin\u2014Madison"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-190.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>This paper presents Eve, a new Execute-Verify architecture that allows state machine replication to scale to multi-core servers. Eve departs from the traditional agree-execute architecture of state machine replication: replicas first execute groups of requests concurrently and then verify that they can reach agreement on a state and output produced by a correct replica; if they can not, they roll back and execute the requests sequentially. Eve minimizes divergence using application-specific criteria to organize requests into groups of requests that are unlikely to interfere. Our evaluation suggests that Eve\u2019s unique ability to combine execution independence with nondetermistic interleaving of requests enables highperformance replication for multi-core servers while tolerating a wide range of faults, including elusive concurrency bugs.</p></div>"], "author": ["Manos Kapritsos and Yang Wang, ", "Vivien Quema, ", " Allen Clement, ", "Lorenzo Alvisi and Mike Dahlin, "], "title": ["All about Eve: Execute-Verify Replication for Multi-Core Servers"], "affiliation": ["University of Texas at Austin; ", "Grenoble INP;", "MPI-SWS; ", "University of Texas at Austin"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-16.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Spanner is Google\u2019s scalable, multi-version, globallydistributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This paper describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free read-only transactions, and atomic schema changes, across all of Spanner.</p></div>"], "author": ["James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi Saito, Michal Szymaniak, Christopher Taylor, Ruth Wang, and Dale Woodford, "], "title": ["Spanner: Google\u2019s Globally-Distributed Database"], "affiliation": ["Google, Inc."]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-162.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Online services distribute and replicate state across geographically diverse data centers and direct user requests to the closest or least loaded site. While effectively ensuring low latency responses, this approach is at odds with maintaining cross-site consistency. We make three contributions to address this tension. First, we propose RedBlue consistency, which enables blue operations to be fast (and eventually consistent) while the remaining red operations are strongly consistent (and slow). Second, to make use of fast operation whenever possible and only resort to strong consistency when needed, we identify conditions delineating when operations can be blue and must be red. Third, we introduce a method that increases the space of potential blue operations by breaking them into separate generator and shadow phases. We built a coordination infrastructure called Gemini that offers RedBlue consistency, and we report on our experience modifying the TPC-W and RUBiS benchmarks and an online social network to use Gemini. Our experimental results show that RedBlue consistency provides substantial performance gains without sacrificing consistency.</p></div>"], "author": ["Cheng Li, ", "Daniel Porto, ", "Allen Clement, ", "Johannes Gehrke, ", "Nuno Pregui\u00e7a and Rodrigo Rodrigues, "], "title": ["Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary"], "affiliation": ["Max Planck Institute for Software Systems; ", "CITI/Universidade Nova de Lisboa and Max Planck Institute for Software Systems; ", "Max Planck Institute for Software Systems; ", "Cornell University; ", "CITI/Universidade Nova de Lisboa"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-4.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Device-driver development and testing is a complex and error-prone undertaking. For example, testing errorhandling code requires simulating faulty inputs from the device. A single driver may support dozens of devices, and a developer may not have access to any of them. Consequently, many Linux driver patches include the comment \u201ccompile tested only.\u201d</p>\r\n<p>SymDrive is a system for testing Linux and FreeBSD drivers without their devices present. The system uses symbolic execution to remove the need for hardware, and extends past tools with three new features. First, SymDrive uses static-analysis and source-to-source transformation to greatly reduce the effort of testing a new driver. Second, SymDrive checkers are ordinary C code and execute in the kernel, where they have full access to kernel and driver state. Finally, SymDrive provides an executiontracing tool to identify how a patch changes I/O to the device and to compare device-driver implementations. In applying SymDrive to 21 Linux drivers and 5 FreeBSD drivers, we found 39 bugs.</p></div>"], "author": ["Matthew J. Renzelmann, Asim Kadav, and Michael M. Swift, "], "title": ["SymDrive: Testing Drivers without Devices"], "affiliation": ["University of Wisconsin\u2014Madison"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-109.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>When systems fail in the field, logged error or warning messages are frequently the only evidence available for assessing and diagnosing the underlying cause. Consequently, the efficacy of such logging\u2014how often and how well error causes can be determined via postmortem log messages\u2014is a matter of significant practical importance. However, there is little empirical data about how well existing logging practices work and how they can yet be improved. We describe a comprehensive study characterizing the efficacy of logging practices across five large and widely used software systems. Across 250 randomly sampled reported failures, we first identify that more than half of the failures could not be diagnosed well using existing log data. Surprisingly, we find that majority of these unreported failures are manifested via a common set of generic error patterns (e.g., system call return errors) that, if logged, can significantly ease the diagnosis of these unreported failure cases. We further mechanize this knowledge in a tool called Errlog, that proactively adds appropriate logging statements into source code while adding only 1.4% performance overhead. A controlled user study suggests that Errlog can reduce diagnosis time by 60.7%.</p></div>"], "author": ["Ding Yuan, ", "Soyeon Park, Peng Huang, Yang Liu, Michael M. Lee, Xiaoming Tang, Yuanyuan Zhou, and Stefan Savage, "], "title": ["Be Conservative: Enhancing Failure Diagnosis with Proactive Logging"], "affiliation": ["University of Illinois at Urbana-Champaign and University of California, San Diego; ", "University of California, San Diego"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-33.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Troubleshooting the performance of production software is challenging. Most existing tools, such as profiling, tracing, and logging systems, reveal what events occurred during performance anomalies. However, users of such tools must infer why these events occurred; e.g., that their execution was due to a root cause such as a specific input request or configuration setting. Such inference often requires source code and detailed application knowledge that is beyond system administrators and end users.</p>\r\n<p>This paper introduces performance summarization, a technique for automatically diagnosing the root causes of performance problems. Performance summarization instruments binaries as applications execute. It first attributes performance costs to each basic block. It then uses dynamic information flow tracking to estimate the likelihood that a block was executed due to each potential root cause. Finally, it summarizes the overall cost of each potential root cause by summing the per-block cost multiplied by the cause-specific likelihood over all basic blocks. Performance summarization can also be performed differentially to explain performance differences between two similar activities. X-ray is a tool that implements performance summarization. Our results show that X-ray accurately diagnoses 17 performance issues in Apache, lighttpd, Postfix, and PostgreSQL, while adding 2.3% average runtime overhead.</p></div>"], "author": ["Mona Attariyan,\u00a0", ";\u00a0Michael Chow and\u00a0Jason Flinn,\u00a0"], "title": ["X-ray: Automating Root-Cause Diagnosis of Performance Anomalies in Production Software"], "affiliation": ["University of Michigan and Google", "University of Michigan", "\u00a0"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-51.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>This paper presents Pasture, a secure messaging and logging library that enables rich mobile experiences by providing secure offline data access. Without trusting users, applications, operating systems, or hypervisors, Pasture leverages commodity trusted hardware to provide two important safety properties: accessundeniability (a user cannot deny any offline data access obtained by his device without failing an audit) and verifiable-revocation (a user who generates a verifiable proof of revocation of unaccessed data can never access that data in the future).</p>\r\n<p>For practical viability, Pasture moves costly trusted hardware operations from common data access actions to uncommon recovery and checkpoint actions. We used Pasture to augment three applications with secure offline data access to provide high availability, rich functionality, and improved consistency. Our evaluation suggests that Pasture overheads are acceptable for these applications.</p></div>"], "author": ["Ramakrishna Kotla and\u00a0Tom Rodeheffer,\u00a0", "\u00a0Indrajit Roy,\u00a0", "Patrick Stuedi, ", "Benjamin Wester,\u00a0"], "title": ["Pasture: Secure Offline Data Access Using Commodity Trusted Hardware"], "affiliation": ["Microsoft Research;", "HP Labs;\u00a0", "IBM Research;\u00a0", "Facebook"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-117.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p><em>Dune</em> is a system that provides applications with direct but safe access to hardware features such as ring protection, page tables, and tagged TLBs, while preserving the existing OS interfaces for processes. Dune uses the virtualization hardware in modern processors to provide a process, rather than a machine abstraction. It consists of a small kernel module that initializes virtualization hardware and mediates interactions with the kernel, and a user-level library that helps applications manage privileged hardware features. We present the implementation of Dune for 64bit x86 Linux. We use Dune to implement three userlevel applications that can benefit from access to privileged hardware: a sandbox for untrusted code, a privilege separation facility, and a garbage collector. The use of Dune greatly simplifies the implementation of these applications and provides significant performance advantages.</p></div>"], "author": ["Adam Belay, Andrea Bittau, Ali Mashtizadeh, David Terei, David Mazi\u00e8res, and Christos Kozyrakis, "], "title": ["Dune: Safe User-level Access to Privileged CPU Features"], "affiliation": ["Stanford University"]},
{"conference": ["OSDI '12 Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/osdi12/osdi12-final-215.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Shared storage services enjoy wide adoption in commercial clouds. But most systems today provide weak performance isolation and fairness between tenants, if at all. Misbehaving or high-demand tenants can overload the shared service and disrupt other well-behaved tenants, leading to unpredictable performance and violating SLAs.</p>\r\n<p>This paper presents Pisces, a system for achieving datacenter-wide per-tenant performance isolation and fairness in shared key-value storage. Today\u2019s approaches for multi-tenant resource allocation are based either on perVM allocations or hard rate limits that assume uniform workloads to achieve high utilization. Pisces achieves per-tenant weighted fair shares (or minimal rates) of the aggregate resources of the shared service, even when different tenants\u2019 partitions are co-located and when demand for different partitions is skewed, time-varying, or bottlenecked by different server resources. Pisces does so by decomposing the fair sharing problem into a combination of four complementary mechanisms\u2014partition placement, weight allocation, replica selection, and weighted fair queuing\u2014that operate on different time-scales and combine to provide system-wide max-min fairness.</p>\r\n<p>An evaluation of our Pisces storage prototype achieves nearly ideal (0.99 Min-Max Ratio) weighted fair sharing, strong performance isolation, and robustness to skew and shifts in tenant demand. These properties are achieved with minimal overhead (&lt;3%), even when running at high utilization (more than 400,000 requests/second/server for 10B requests).</p></div>"], "author": ["David Shue and\u00a0Michael J. Freedman,\u00a0", "Anees Shaikh,\u00a0"], "title": ["Performance Isolation and Fairness for Multi-Tenant Cloud Storage"], "affiliation": ["Princeton University;\u00a0", "IBM T.J. Watson Research Center"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/mad12/mad12-final1.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">System data is abundant, yet data-driven decision making\u00a0is currently more of an art than a science. Many organizations\u00a0rely on data analysis for problem detection and\u00a0diagnosis, but the process continues to be custom and ad\u00a0hoc. In this paper, we examine the analytics process undertaken\u00a0by users to mine large data sets, and try to characterize\u00a0these searches by the operations performed. Furthermore,\u00a0we take a first stab at a methodical process to\u00a0automatically suggest operations based on statistical analysis\u00a0of previous searches performed.</p></div>"], "author": ["Sara Alspaugh, ", " Archana Ganapathi, "], "title": ["Towards a Data Analysis Recommendation System"], "affiliation": ["University of California, Berkeley;", "Splunk, Inc."]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/mad12-final11.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Software upgrades are frequent. Unfortunately, many of the\u00a0upgrades either fail or misbehave. We argue that many of\u00a0these failures can be avoided for new users of each upgrade\u00a0by exploiting the characteristics of the upgrade and feedback\u00a0from the users that have already installed it. To demonstrate\u00a0that this can be achieved, we build Mojave, the first recommendation\u00a0system for software upgrades. Mojave leverages\u00a0data from the existing and new users, machine learning, and\u00a0static and dynamic source analyses. For each new user, Mojave\u00a0computes the likelihood that the upgrade will fail for\u00a0him/her. Based on this value, Mojave recommends for or\u00a0against the upgrade. We evaluate Mojave for two real upgrade\u00a0problems with the OpenSSH suite. Initial results show\u00a0that it provides accurate recommendations.</p></div>"], "author": ["Rekha Bachwani, ", " Olivier Crameri, ", " Ricardo Bianchini, ", " Willy Zwaenepoel, "], "title": ["Mojave: A Recommendation System for Software Upgrades"], "affiliation": ["Rutgers University;", "EPFL;", "Rutgers University;", "EPFL"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/mad12/mad12-final2.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">In this paper we describe Vayu, a system for managing cloud applications from the performance, availability and capacity standpoints. The system automatically learns the behavior of cloud applications and the remediation actions required to avoid and resolve problems that may arise in the application by detecting and creating signatures of problems and mapping them to a finite set of automated remediation actions available in cloud environments. Vayu is based on a set of algorithms; anomaly detection, problem signature and similarity and classification based action learning.\u00a0</p></div>"], "author": ["Ira Cohen, Ohad Assulin, Eli Mordechai, Yaniv Sayers, and Ruth Bernstein, "], "title": ["Vayu: Learning to Control the Cloud"], "affiliation": ["HP Software"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/mad12/mad12-final4.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">In modern computing facilities, higher and higher operating\u00a0temperatures are due to the employment of power-hungry\u00a0devices, hence the need for cost-effective heat dissipation\u00a0solutions to guarantee proper operating temperatures.\u00a0Within this context, dynamic thermal management\u00a0techniques (DTM) can be highly beneficial in proactively\u00a0control heat dissipation, avoiding overheating. The large-scale\u00a0adoption of DTM may eventually allow the use of\u00a0more cost-effective heat dissipation system, with great\u00a0power consumption advantages for large datacenters.</p>\r\n<p class=\"p1\">Preventive thermal management is a technique to\u00a0achieve long-term thermal control via performance degradation.\u00a0However, this may result in impaired Quality\u00a0of Service (QoS) and Service Level Agreements (SLAs)\u00a0breaking. We address this problem by proposing a self-adaptive\u00a0framework combining performance and thermal\u00a0management targeting Chip Multi-Processors (CMPs).\u00a0The proposed methodology harnesses control-theoretical\u00a0controllers for driving idle-cycle injection and threads priority\u00a0adjustment, in order to provide control over the processor\u00a0temperature, while taking applications\u2019 QoS (in\u00a0terms of performance) into account. We implemented our\u00a0framework in the FreeBSD operating system and evaluated\u00a0it on real hardware, also comparing it with a previous\u00a0framework for preventive DPTM.\u00a0</p></div>"], "author": ["Davide Basilio Bartolini, ", " Filippo Sironi, ", " Martina Maggio, ", " Riccardo Cattaneo and Donatella Sciuto, ", "Marco Domenico Santambrogio,\u00a0"], "title": ["A Framework for Thermal and Performance Management"], "affiliation": ["Politecnico di Milano;", "Massachusetts Institute of Technology;", "Lund University;", "Politecnico di Milano;\u00a0", "Politecnico di Milano and\u00a0"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/mad12/mad12-final7.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Problem diagnosis and debugging in distributed environments\u00a0such as the cloud and popular distributed systems\u00a0frameworks has been a hard problem. We explore an\u00a0evaluation of a novel way of debugging distributed systems,\u00a0such as the MapReduce framework, by using system\u00a0calls. Performance problems in such systems can\u00a0be hard to diagnose and to localize to a specific node\u00a0or a set of nodes. Additionally, most debugging systems\u00a0often rely on forms of instrumentation and signatures\u00a0that sometimes cannot truthfully represent the state\u00a0of the system (logs or application traces for example).\u00a0We focus on evaluating the performance debugging of\u00a0these frameworks using a low level of abstraction - system\u00a0calls. By focusing on a small set of system calls, we\u00a0try to extrapolate meaningful information on the control\u00a0flow and state of the framework, providing accurate and\u00a0meaningful automated debugging.</p></div>"], "author": ["Nikhil Khadke, Michael P. Kasick, Soila P. Kavulya, Jiaqi Tan, and Priya Narasimhan, "], "title": ["Transparent System Call Based Performance Debugging for Cloud Computing"], "affiliation": ["Carnegie Mellon University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/mad12/mad12-final6.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">A promising way to capture the characteristics of changing\u00a0traffic is to extract significant flow clusters in traffic.\u00a0However, clustering flows by 5-tuple requires flow\u00a0matching in huge flow attribute spaces, and thus, is difficult\u00a0to perform on the fly. We propose an efficient yet\u00a0flexible flow aggregation technique for monitoring the\u00a0dynamics of network traffic. Our scheme employs two-stage\u00a0flow-aggregation. The primary aggregation stage\u00a0is for efficiently processing a huge volume of raw traffic\u00a0records. It first aggregates each attribute of 5-tuple\u00a0separately, and then, produces multi-dimensional flows\u00a0by matching each attribute of a flow to the resulted aggregated\u00a0attributes. The secondary aggregation stage is\u00a0for providing flexible views to operators. It performs\u00a0multi-dimensional aggregation with the R-tree algorithm\u00a0to produce concise summaries for operators. We report\u00a0our prototype implementation and preliminary results using\u00a0traffic traces from backbone networks.</p></div>"], "author": ["Midori Kato, ", " Kenjiro Cho, ", " Michio Honda, ", " Hideyuki Tokuda, "], "title": ["Monitoring the Dynamics of Network Traffic by Recursive Multi-Dimensional Aggregation"], "affiliation": ["Keio University;", "IIJ/Keio University;", "NEC Europe Ltd.;", "Keio University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/mad12-final8.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Supercomputer components are inherently stateful\u00a0and interdependent, so accurate assessment of an event on one\u00a0component often requires knowledge of previous events on that\u00a0component or others. Administrators who daily monitor and\u00a0interact with the system generally possess sufficient operational\u00a0context to accurately interpret events, but researchers with only\u00a0historical logs are at risk for incorrect conclusions. To address this\u00a0risk, we present a state-machine approach for tracing context in\u00a0event logs, a flexible implementation in Splunk, and an example\u00a0of its use to disambiguate a frequently occurring event type on\u00a0an extreme-scale supercomputer. Specifically, of 70,126 heartbeat\u00a0stop events over three months, we identify only 2% as indicating\u00a0failures.</p></div>"], "author": ["Jon Stearley, Robert Ballance, and Lara Bauman, "], "title": ["A State-Machine Approach to Disambiguating Supercomputer Event Logs"], "affiliation": ["Sandia National Laboratories"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/mad12-final9.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Tracing mechanisms in distributed systems give important\u00a0insight into system properties and are usually sampled\u00a0to control overhead. At Google, Dapper [8] is\u00a0the always-on system for distributed tracing and performance\u00a0analysis, and it samples fractions of all RPC traffic.\u00a0Due to difficult implementation, excessive data volume,\u00a0or a lack of perfect foresight, there are times when\u00a0system quantities of interest have not been measured directly,\u00a0and Dapper samples can be aggregated to estimate\u00a0those quantities in the short or long term. Here we\u00a0find unbiased variance estimates of linear statistics over\u00a0RPCs, taking into account all layers of sampling that occur\u00a0in Dapper, and allowing us to quantify the sampling\u00a0uncertainty in the aggregate estimates. We apply this\u00a0methodology to the problem of assigning jobs and data\u00a0to Google datacenters, using estimates of the resulting\u00a0cross-datacenter traffic as an optimization criterion, and\u00a0also to the detection of change points in access patterns\u00a0to certain data partitions.</p></div>"], "author": ["Nate Coehlo, Arif Merchant, and Murray Stokely, "], "title": ["Uncertainty in Aggregate Estimates from Sampled Distributed Traces"], "affiliation": ["Google, Inc."]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final1.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Web applications are fundamentally reactive. Code in a <span class=\"s1\">web page runs in reaction to events, which are triggered either by external stimuli or by other events. The DOM, </span>which specifies these behaviors, is therefore central to the behavior of web applications. We define the first formal model of event behavior in the DOM, with high <span class=\"s1\">fidelity to the DOM specification. Our model is concise </span><span class=\"s2\">and executable, and can therefore be used for testing and </span>verification. We have applied it in several settings: to <span class=\"s1\">establish some intended meta-properties of the DOM, as </span>an oracle for testing the behavior of browsers (where it <span class=\"s1\">found real errors), to demonstrate unwanted interactions </span><span class=\"s2\">between extensions and validate corrections to them, and </span>to examine the impact of a web sandbox. The model <span class=\"s3\">composes easily with models of other web components, </span><span class=\"s1\">as a step toward full formal modeling of the web.</span></p></div>"], "author": ["Benjamin S. Lerner, Matthew J. Carroll, Dan P. Kimmel,\u00a0Hannah Quay-de la Vallee, and Shriram Krishnamurthi,\u00a0"], "title": ["Modeling and Reasoning about DOM Events"], "affiliation": ["Brown University"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final8.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">A web application often includes content from a variety of origins. Securing such a mashup application is challenging because origins often distrust each other and wish to expose narrow interfaces to their private code and data. Jigsaw is a new framework for isolating these mashup components. Jigsaw is an extension of the JavaScript language that can be run inside standard browsers using a Jigsaw-to-JavaScript compiler. Unlike prior isolation schemes that require developers to specify complex, error-prone policies, Jigsaw leverages the well-understood public/private keywords from traditional object-oriented languages, making it easy for a domain to tag internal data as externally visible. Jigsaw provides strong iframe-like isolation, but unlike previous approaches that use actual iframes as isolation containers, Jigsaw allows mutually distrusting code to run inside the same frame; this allows scripts to share state using synchronous method calls instead of asynchronous message passing. Jigsaw also introduces a novel encapsulation mechanism called surrogates. Surrogates allow domains to safely exchange objects by reference instead of by value. This improves sharing efficiency by eliminating cross-origin marshaling overhead.</p></div>"], "author": ["James Mickens, ", "\u00a0Matthew Finifter, "], "title": ["Jigsaw: Ef\ufb01cient, Low-effort Mashup Isolation"], "affiliation": ["Microsoft Research;", "University of California, Berkeley"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final13.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Mapping user profiles across social network sites enables sharing and interactions between social networks, which enriches the social networking experience. Manual mapping for user profiles is a time consuming and tedious task. In addition profile mapping algorithms are inaccurate and are usually based on simple name or email string matching. In this paper, we propose a Game With A Purpose (GWAP) approach to solve the profile mapping problem. The proposed approach leverages the game appeal and social community to generate the profile mappings. We designed and implemented an online social networking game (<em>GameMapping</em>), the game is fun and is based on human verification. The game presents the players with some profiles information, and uses human computation and knowledge about the information being presented to map similar user profiles. The game was modeled using incomplete information game theory, and a proof of sequential equilibrium was provided. To test the effectiveness of the mapping technique and detection strategies, the game was implemented and deployed on Facebook, MySpace and Twitter and the experiments were performed on the real data collected from users playing the game.</p></div>"], "author": ["Mohamed Shehab, Moo Nam Ko, and Hakim Touati,\u00a0"], "title": ["Social Networks Pro\ufb01le Mapping Using Games"], "affiliation": ["University of North Carolina at Charlotte"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final18.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Platform-as-a-service (PaaS) systems, such as Google App Engine (GAE), simplify web application development and cloud deployment by providing developers with complete software stacks: runtime systems and scalable services accessible from well-defined APIs. Extant PaaS offerings are designed and specialized to support large numbers of concurrently executing web applications (multi-tier programs that encapsulate and integrate business logic, user interface, and data persistence). To enable this, PaaS systems impose a programming model that places limits on available library support, execution duration, data access, and data persistence. Although successful and scalable for web services, such support is not as amenable to online analytical processing (OLAP), which have variable resource requirements and require greater flexibility for ad-hoc query and data analysis. OLAP of web applications is key to understanding how programs are used in live settings.</p> <p class=\"p1\">In this work, we empirically evaluate OLAP support in the GAE public cloud, discuss its benefits, and limitations. We then present an alternate approach, which combines the scale of GAE with the flexibility of customizable offline data analytics. To enable this, we build upon and extend the AppScale PaaS \u2013 an open source private cloud platform that is API-compatible with GAE. Our approach couples GAE and AppScale to provide a hybrid cloud that transparently shares data between public and private platforms, and decouples public application execution from private analytics over the same datasets. Our extensions to AppScale eliminate the restrictions GAE imposes and integrates popular data analytic programming models to provide a framework for complex analytics, testing, and debugging of live GAE applications with low overhead and cost.</p></div>"], "author": ["Navraj Chohan, Anand Gupta, Chris Bunch, Kowshik Prakasam, and Chandra Krintz, "], "title": ["Hybrid Cloud Support for Large Scale Analytics and Web Processing"], "affiliation": ["University of California, Santa Barbara"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final16.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Typical social networking functionalities such as feed following are known to be hard to scale. Different from the popular approach that sacrifices consistency for scalability, in this paper we describe, implement, and evaluate a method that can simultaneously achieve scalability and consistency in feed following applications built on shared-nothing distributed systems. Timing and client-side processing are the keys to this approach. Assuming global time is available at all the clients and servers, the distributed servers publish a pre-agreed upon schedule based on which the continuously committed updates are periodically released for read. This opens up opportunities for caching and client-side processing, and leads to scalability improvements. This approach trades freshness for scalability.</p><p class=\"p1\">Following this approach, we build a twitter-style feed following application and evaluate it on a following network with about 200,000 users under synthetic workloads. The resulting system exhibits linear scalability in our experiment. With 6 low-end cloud instances costing a total of no more than $1.2 per hour, we recorded a peak timeline query rate at about 10 million requests per day, under a fixed update rate of 1.6 million new tweets per day. The maximum staleness of the responses is 5 seconds. The performance achieved sufficiently verifies the feasibility of this approach, and provides an alternative to build small to medium size social networking applications on the cheap.</p></div>"], "author": ["Zhiwu Xie,\u00a0", "\u00a0Jinyang Liu,\u00a0", "\u00a0Herbert Van de Sompel,\u00a0", "\u00a0Johann van Reenen and Ramiro Jordan,\u00a0"], "title": ["Poor Man's Social Network: Consistently Trade Freshness for Scalability"], "affiliation": ["Virginia Polytechnic Institute and State University;", "Howard Hughes Medical Institute;", "Los Alamos National Laboratory;", "University of New Mexico"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final4.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\"><span class=\"s1\">Partitioning data over multiple storage servers is an attrac</span>tive way to increase throughput for web-like workloads. However, there is often no one partitioning that yields good performance for all queries, and it can be challeng<span class=\"s1\">ing for the web developer to determine how best to execute </span>queries over partitioned data.</p> <p class=\"p1\"><span class=\"s2\">This paper presents D</span><span class=\"s3\">IXIE</span><span class=\"s2\">, a SQL query planner, optimizer, and executor for databases horizontally partitioned </span>over multiple servers. D<span class=\"s3\">IXIE </span>focuses on increasing inter-<span class=\"s1\">query parallel speedup by involving as few servers as pos</span>sible in each query. One way it does this is by supporting tables with multiple copies partitioned on different <span class=\"s2\">columns, in order to expand the set of queries that can be satisified from a single server. D</span><span class=\"s4\">IXIE </span><span class=\"s2\">automatically transforms SQL queries to execute over a partitioned database, </span><span class=\"s1\">using a cost model and plan generator that exploit multiple </span>table copies.</p> <p class=\"p1\">We evaluate D<span class=\"s3\">IXIE </span>on a database and query stream taken from Wikipedia, partitioned across ten MySQL servers. By adding one copy of a 13 MB table and using D<span class=\"s3\">IXIE</span>\u2019s query optimizer, we achieve a throughput <span class=\"s1\">improvement of 3.2X over a single optimized partitioning </span>of each table and 8.5X over the same data on a single server. On specific queries D<span class=\"s3\">IXIE </span>with table copies increases throughput linearly with the number of servers, <span class=\"s1\">while the best single-table-copy partitioning achieves little </span>scaling. For a large class of joins, which traditional wis<span class=\"s2\">dom suggests requires tables partitioned on the join keys, D</span><span class=\"s4\">IXIE </span><span class=\"s2\">can find higher-performance plans using other par</span>titionings.</p></div>"], "author": ["Neha Narula and\u00a0Robert Morris,\u00a0"], "title": ["Executing Web Application Queries on a Partitioned Database"], "affiliation": ["MIT CSAIL"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final7.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Gibraltar is a new framework for exposing hardware devices to web pages. Gibraltar\u2019s fundamental insight is that JavaScript\u2019s AJAX facility can be used as a hardware access protocol. Instead of relying on the browser to mediate device interactions, Gibraltar sandboxes the browser and uses a small device server to handle hardware requests. The server uses native code to interact with devices, and it exports a standard web server interface on the localhost. To access hardware, web pages send device commands to the server using HTTP requests; the server returns hardware data via HTTP responses.</p> <p class=\"p1\">Using a client-side JavaScript library, we build a simple yet powerful device API atop this HTTP transfer protocol. The API is particularly useful to developers of mobile web pages, since mobile platforms like cell phones have an increasingly wide array of sensors that, prior to Gibraltar, were only accessible via native code plugins or the limited, inconsistent APIs provided by HTML5. Our implementation of Gibraltar on Android shows that Gibraltar provides stronger security guarantees than HTML5; furthermore, it shows that HTTP is responsive enough to support interactive web pages that perform frequent hardware accesses. Gibraltar also supports an HTML5 compatibility layer that implements the HTML5 interface but provides Gibraltar\u2019s stronger security.</p></div>"], "author": ["Kaisen Lin,\u00a0", "\u00a0David Chu, \u00a0James Mickens,\u00a0Li Zhuang, and Feng Zhao,\u00a0", "\u00a0Jian Qiu,\u00a0"], "title": ["Gibraltar: Exposing Hardware Devices to Web Pages Using AJAX"], "affiliation": ["UC San Diego;", "Microsoft Research;", "National University of Singapore"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final5.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Traditional web-based client-server application development has been accomplished in two separate pieces: the frontend portion which runs on the client machine has been written in HTML and JavaScript; and the backend portion which runs on the server machine has been written in PHP, ASP.net, or some other \u201cserver-side\u201d language which typically interfaces to a database. The skill sets required for these two pieces are different.</p> <p class=\"p1\">In this paper, I demonstrate a new methodology for web-based client-server application development, in which a simulated server is built into the browser environment to run the backend code. This allows the frontend to issue requests to the backend, and the developer to step, using a debugger, directly from frontend code into backend code, and to debug and test both the frontend and backend portions. Once working, that backend code is moved to a real server. Since the application-specific code has been tested in the simulated environment, it is unlikely that bugs will be encountered at the server that did not exist in the simulated environment.</p> <p class=\"p1\">I have implemented this methodology and used it for development of a live application. All of the code is open source.</p></div>"], "author": ["Derrell Lipman,\u00a0"], "title": ["LIBERATED: A Fully In-Browser Client and Server Web Application Debug and Test Environment"], "affiliation": ["University of Massachusetts Lowell"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final6.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Running on billions of today\u2019s computing devices, JavaScript has become a ubiquitous platform for deploying web applications. Unfortunately, an application developer who wishes to include a third-party script must enter into an implicit trust relationship with the third-party\u2014granting it unmediated access to its entire application content.</p> <p class=\"p1\">In this paper, we present js.js, a JavaScript interpreter (which runs in JavaScript) that allows an application to execute a third-party script inside a completely isolated, sandboxed environment. An application can, at runtime, create and interact with the objects, properties, and methods available from within the sandboxed environment, giving it complete control over the third-party script. js.js supports the full range of the JavaScript language, is compatible with major browsers, and is resilient to attacks from malicious scripts.</p> <p class=\"p1\">We conduct a performance evaluation quantifying the overhead of using js.js and present an example of using js.js to execute Twitter\u2019s Tweet Button API.</p><p class=\"p1\">\u00a0</p></div>"], "author": ["Jeff Terrace, Stephen R. Beard, and Naga Praveen Kumar Katta,\u00a0", "View the\u00a0", "."], "title": ["JavaScript in JavaScript (js.js): Sandboxing Third-Party Scripts"], "affiliation": ["Princeton University"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final12.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Twitter has become a persistent part of our digital lives, connecting us not only to our individual audiences but also to an entire landscape of applications built for the web. While much has been done to support the Twitter ecosystem outside of Twitter, little has been done within Twitter to power those same applications. This work introduces a service called <em>Aperator</em>, which supports application-specific actionable commands through tweets. This ability creates several interesting opportunities for both end-users and application developers building on the Twitter platform. For example, the actionable command capability allows a link that a Twitter user shares with his followers to be directly added to any of the user\u2019s connected link sharing networks, such as Delicious or Read it Later. The client side of this system has a console for end-users to sign up and provide their login credentials for various web services that our system supports: Delicious, Foursquare, Read it Later, Foursquare etc. The system\u2019s backend has two cron jobs that run every minute to: (a) retrieve and parse tweets from a specific twitter account and store them in a command form in a MySQL database, and (b) execute the unexecuted commands found in the users tweets. This paper describes the concept, implementation, and results from an experimental study of this new application.</p></div>"], "author": ["Peter Zakin,\u00a0Soumya Sen, and\u00a0Mung Chiang,\u00a0"], "title": ["Aperator: Making Tweets Enable Actionable Commands on Third Party Web Applications"], "affiliation": ["Princeton University"]},
{"conference": ["WebApps '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/webapps12/webapps12-final19.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>We outline the groundwork for a new software development approach where developers author the server-side application logic and rely on tools to automatically synthesize the corresponding client-side application logic. Our approach uses program analysis techniques to extract a logical speci\ufb01cation from the server and synthesizes client code from that speci\ufb01cation. Our implementation (WAVES) synthesizes interactive client interfaces that include asynchronous callbacks whose performance and coverage rival that of manually written clients, while ensuring that no new security vulnerabilities are introduced.</p></div>"], "author": ["Nazari Skrupsky, Maliheh Monshizadeh, Prithvi Bisht, Timothy Hinrichs, V.N. Venkatakrishnan, and Lenore Zuck,\u00a0"], "title": ["Don\u2019t Repeat Yourself: Automatically Synthesizing Client-side Validation Code for Web Applications"], "affiliation": ["University of Illinois at Chicago"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final283.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\"><span class=\"s1\">The high degree of storage consolidation in modern virtualized datacenters requires flexible and efficient ways to allocate IO resources among virtual machines (VMs). Existing IO resource management techniques have two </span>main deficiencies: (1) they are restricted in their ability to allocate resources across multiple hosts sharing a storage device, and (2) they do not permit the administrator to set allocations for a group of VMs that are providing a single <span class=\"s1\">service or belong to the same application.</span></p> <p class=\"p2\"><span class=\"s2\">In this paper we present the design and implementation of a novel software system called <em>Storage Resource Pools</em> (SRP). SRP supports the logical grouping of related VMs </span>into hierarchical pools. SRP allows reservations, limits and proportional shares, at both the VM and pool levels. <span class=\"s2\">Spare resources are allocated to VMs in the same pool in </span>preference to other VMs. The VMs may be distributed across multiple physical hosts without consideration of their logical groupings.<span> </span>We have implemented a proto<span class=\"s2\">type of storage resource pools in the VMware ESX hyper</span>visor. Our results demonstrate that SRP provides hierarchical performance isolation and sharing among groups <span class=\"s2\">of VMs running across multiple hosts, while maintaining </span><span class=\"s1\">high utilization of the storage device.</span></p><p>\u00a0</p></div>"], "author": ["Ajay Gulati and Ganesha Shanmuganathan,\u00a0", "\u00a0Xuechen Zhang,\u00a0", "\u00a0Peter Varman,\u00a0"], "title": ["Demand Based Hierarchical QoS Using Storage Resource Pools"], "affiliation": ["VMware Inc.;", "Wayne State University;", "Rice University"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final181_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Windows Azure Storage (WAS) is a cloud storage system that provides customers the ability to store seemingly limitless amounts of data for any duration of time. WAS customers have access to their data from anywhere, at any time, and only pay for what they use and store. To provide durability for that data and to keep the cost of storage low, WAS uses erasure coding.</p> <p class=\"p1\">In this paper we introduce a new set of codes for erasure coding called Local Reconstruction Codes (LRC). LRC reduces the number of erasure coding fragments that need to be read when reconstructing data fragments that are offline, while still keeping the storage overhead low. The important benefits of LRC are that it reduces the bandwidth and I/Os required for repair reads over prior codes, while still allowing a significant reduction in storage overhead. We describe how LRC is used in WAS to provide low overhead durable storage with consistently low read latencies.</p><p>\u00a0</p></div>"], "author": ["Cheng Huang, Huseyin Simitci, Yikang Xu, Aaron Ogus, Brad Calder, Parikshit Gopalan, Jin Li, and Sergey Yekhanin,\u00a0", "\n\u00a0\u00a0\u00a0\u00a0"], "title": ["Erasure Coding in Windows Azure Storage"], "affiliation": ["Microsoft Corporation"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final206.pdf", "https://www.usenix.org/system/files/conference/atc12/atc12-final206-7-20-12.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Distributed systems designs often employ replication to solve two different kinds of availability problems. First, to prevent the loss of data through the permanent destruction or disconnection of a distributed node, and second, to allow prompt retrieval of data when some distributed nodes respond slowly. For simplicity, many systems further handle crash-restart failures and timeouts by treating them as a permanent disconnection followed by the birth of a new node, relying on peer replication rather than persistent storage to preserve data. We posit that for applications deployed in modern managed infrastructures, delays are typically transient and failed processes and machines are likely to be restarted promptly, so it is often desirable to resume crashed processes from persistent checkpoints. In this paper we present MaceKen, a synthesis of complementary techniques including Ken, a lightweight and decentralized rollback-recovery protocol that transparently masks crash-restart failures by careful handling of messages and state checkpoints; and Mace, a programming toolkit supporting development of distributed applications and application-specific availability via replication. MaceKen requires near-zero additional developer effort\u2014systems implemented in Mace can immediately benefit from the Ken protocol by virtue of following the Mace execution model. Moreover, this model allows multiple, independently developed application components to be seamlessly composed, preserving strong global reliability guarantees. Our implementation is available as open source software.</p><p>\u00a0</p></div>"], "author": ["Sunghwan Yoo, ", "\u00a0 Charles Killian,\u00a0", "\u00a0Terence Kelly,\u00a0", "\u00a0Hyoun Kyu Cho,\u00a0", "\u00a0Steven Plite,\u00a0"], "title": ["Composable Reliability for Asynchronous Systems"], "affiliation": ["Purdue University and ", ";", "Purdue University;", "HP Labs;", "HP Labs and\u00a0", "University of Michigan;", "Purdue University"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final182.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Grace is a graph-aware, in-memory, transactional graph management system, specifically built for real-time queries and fast iterative computations. It is designed to run on large multi-cores, taking advantage of the inherent parallelism to improve its performance. Grace contains a number of graph-specific and multi-core-specific optimizations including graph partitioning, careful in-memory vertex ordering, updates batching, and load-balancing. It supports queries, searches, iterative computations, and transactional updates. Grace scales to large graphs (e.g., a Hotmail graph with 320 million vertices) and performs up to two orders of magnitude faster than commercial key-value stores and graph databases.</p><p>\u00a0</p></div>"], "author": ["Vijayan Prabhakaran, Ming Wu, Xuetian Weng, Frank McSherry, Lidong Zhou, and Maya Haridasan, "], "title": ["Managing Large Graphs on Multi-Cores with Graph Awareness"], "affiliation": ["Microsoft Research"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final229.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Modern multicore systems are based on a Non-Uniform Memory Access (NUMA) design. Efficiently exploiting such architectures is notoriously complex for programmers. One of the key concerns is to limit as much as possible the number of remote memory accesses (i.e., main memory accesses performed from a core to a memory bank that is not directly attached to it). However, in many cases, existing profilers do not provide enough information to help programmers achieve this goal.</p> <p class=\"p1\">This paper presents MemProf, a profiler that allows programmers to choose and implement efficient application-level optimizations for NUMA systems. MemProf builds temporal flows of interactions between threads and objects, which help programmers understand why and which memory objects are accessed remotely. We evaluate MemProf on Linux using four applications (FaceRec, Streamcluster, Psearchy, and Apache) on three different machines. In each case, we show how MemProf helps us choose and implement efficient optimizations, unlike existing profilers. These optimizations provide significant performance gains (up to 161%), while requiring very lightweight modifications (10 lines of code or less).</p><p>\u00a0</p></div>"], "author": ["Renaud Lachaize,\u00a0", "\u00a0Baptiste Lepers,\u00a0", "\u00a0Vivien Qu\u00e9ma,\u00a0"], "title": ["MemProf: A Memory Pro\ufb01ler for NUMA Multicore Systems"], "affiliation": ["UJF;", "CNRS;", "GrenobleINP"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final237.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\"><span class=\"s1\">The scalability of multithreaded applications on current multicore systems is hampered by the performance of </span>lock algorithms, due to the costs of access contention and <span class=\"s2\">cache misses. In this paper, we propose a new lock algo</span>rithm, Remote Core Locking (RCL), that aims to improve the performance of critical sections in legacy applications <span class=\"s3\">on multicore architectures. The idea of RCL is to replace </span><span class=\"s1\">lock acquisitions by optimized remote procedure calls to a dedicated server core. RCL limits the performance </span><span class=\"s3\">collapse observed with other lock algorithms when many </span><span class=\"s1\">threads try to acquire a lock concurrently and removes </span><span class=\"s3\">the need to transfer lock-protected shared data to the core </span>acquiring the lock because such data can typically remain <span class=\"s2\">in the server core\u2019s cache.</span></p> <p class=\"p2\">We have developed a profiler that identifies the locks <span class=\"s3\">that are the bottlenecks in multithreaded applications and </span><span class=\"s2\">that can thus benefit from RCL, and a reengineering tool </span>that transforms POSIX locks into RCL locks. We have <span class=\"s2\">evaluated our approach on 18 applications: Memcached, </span><span class=\"s3\">Berkeley DB, the 9 applications of the SPLASH-2 bench</span><span class=\"s2\">mark suite and the 7 applications of the Phoenix2 bench</span>mark suite. 10 of these applications, including Mem<span class=\"s2\">cached and Berkeley DB, are unable to scale because of </span>locks, and benefit from RCL. Using RCL locks, we get <span class=\"s3\">performance improvements of up to 2.6 times with respect </span><span class=\"s2\">to POSIX locks on Memcached, and up to 14 times with respect to Berkeley DB.</span></p><p>\u00a0</p></div>"], "author": ["Jean-Pierre Lozi, Florian David, Ga\u00ebl Thomas, Julia Lawall, and Gilles Muller,\u00a0"], "title": ["Remote Core Locking: Migrating Critical-Section Execution to Improve the Performance of Multithreaded Applications"], "affiliation": ["LIP6/INRIA"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final16.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">High Level Synthesis (HLS) is a promising technology where algorithms described in high level languages are automatically transformed into a hardware design. Although many HLS tools exist, they are mainly targeting developers who want to use a high level programming language to design hardware modules. They are not designed to automatically compile a complete software system, such as a network packet processing application, into a hardware design.</p> <p class=\"p1\">In this paper, we describe a compiler toolchain that automatically transforms existing software in a limited domain to a functional hardware design. We have selected the Click Modular Router as the input system, and the Stanford NetFPGA as the target hardware platform. Our toolchain uses LLVM to transform Click C++ code into a form suitable for hardware implementation and then uses AHIR, a high level synthesis toolchain, to produce a VHDL netlist.</p> <p class=\"p1\">The resulting netlist has been verified with actual hardware on the NetFPGA platform. The resulting hardware can achieve 20-50 % of the performance compared to version handwritten in Verilog. We expect that improvements on the toolchain could provide better performance, but for the first prototype the results are good. We feel that one of the biggest contribution of this work is that it shows some new principles of high-level synthesis that could also be applied to different domains, source languages and targets.</p><p>\u00a0</p></div>"], "author": ["Teemu Rinta-aho and\u00a0Mika Karlstedt,\u00a0", "\u00a0Madhav P. Desai,\u00a0"], "title": ["The Click2NetFPGA Toolchain"], "affiliation": ["NomadicLab, Ericsson Research;", "Indian Institute of Technology (Bombay)"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final121.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">We aim at improving the power efficiency of network routers without compromising their performance. Using server-based software routers as our prototyping vehicle, we investigate the design of a router that consumes power in proportion to the rate of incoming traffic. We start with an empirical study of power consumption in current software routers, decomposing the total power consumption into its component causes. Informed by this analysis, we develop software mechanisms that exploit the underlying hardware\u2019s power management features for more energy-efficient packet processing. We incorporate these mechanisms into Click and demonstrate a router that matches the peak performance of the original (unmodified) router while consuming up to half the power at low loads, with negligible impact on the packet forwarding latency.</p><p>\u00a0</p></div>"], "author": ["Luca Niccolini,\u00a0", "\u00a0Gianluca Iannaccone,\u00a0", "\u00a0Sylvia Ratnasamy,\u00a0", "\u00a0Jaideep Chandrashekar,\u00a0", "\u00a0Luigi Rizzo,\u00a0"], "title": ["Building a Power-Proportional Software Router"], "affiliation": ["University of Pisa;", "RedBow Labs;", "University of California, Berkeley;", "Technicolor Labs;", "University of Pisa and\u00a0"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final186.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Many applications (routers, traffic monitors, firewalls, etc.) need to send and receive packets at line rate even on very fast links. In this paper we present <em>netmap</em>, a novel framework that enables commodity operating systems to handle the millions of packets per seconds traversing 1..10 Gbit/s links, without requiring custom hardware or changes to applications.</p> <p class=\"p1\">In building <em>netmap</em>, we identified and successfully reduced or removed three main packet processing costs: per-packet dynamic memory allocations, removed by preallocating resources; system call overheads, amortized over large batches; and memory copies, eliminated by sharing buffers and metadata between kernel and userspace, while still protecting access to device registers and other kernel memory areas. Separately, some of these techniques have been used in the past. The novelty in our proposal is not only that we exceed the performance of most of previous work, but also that we provide an architecture that is tightly integrated with existing operating system primitives, not tied to specific hardware, and easy to use and maintain.</p> <p class=\"p1\"><em>netmap </em>has been implemented in FreeBSD and Linux for several 1 and 10 Gbit/s network adapters. In our prototype, a single core running at 900 MHz can send or receive 14.88 Mpps (the peak packet rate on 10 Gbit/s links). This is more than 20 times faster than conventional APIs. Large speedups (5x and more) are also achieved on user-space Click and other packet forwarding applications using a libpcap emulation library running on top of <em>netmap</em>.</p><p>\u00a0</p></div>"], "author": ["Luigi Rizzo, ", "\n\u00a0\u00a0\u00a0\u00a0"], "title": ["netmap: A Novel Framework for Fast Packet I/O"], "affiliation": ["Universit\u00e0 di Pisa", ", Italy"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final263.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Forensic analysts typically require access to application-layer information gathered over long periods of time to completely investigate network security incidents. Unfortunately, storing longitudinal network data is often at odds with maintaining detailed payload information due to the overhead associated with storing and querying such data. Thus, the analyst is left to choose between coarse information about long-term network activities or brief glimpses of detailed attack activity. In this paper, we take the first steps toward a storage framework for network payload information that provides a better balance between these two extremes. We take advantage of the redundancy found in network data to aggregate payload information into flexible and efficiently compressible data objects that are associated with network flows. To enable interactive querying, we introduce a hierarchical indexing structure for both the flow and payload information, which allows us to quickly prune irrelevant data and answer queries directly from the indexing information. Our empirical results on data collected from a campus network show that our approach can significantly reduce the volume of the stored data, while simultaneously preserving the ability to perform detailed queries with response times on the order of seconds.</p><p>\u00a0</p></div>"], "author": ["Teryl Taylor,\u00a0", "\u00a0Scott E. Coull,\u00a0", "\u00a0Fabian Monrose,\u00a0", "\u00a0John McHugh,\u00a0"], "title": ["Toward Ef\ufb01cient Querying of Compressed Network Payloads"], "affiliation": ["UNC Chapel Hill;", "RedJack;", "UNC Chapel Hill;", "RedJack"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final44.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\"><em>BinArmor </em>is a novel technique to protect existing C binaries from memory corruption attacks on both control data and non-control data. Without access to source code, non-control data attacks cannot be detected with current techniques. Our approach hardens binaries against both kinds of overflow, without requiring the programs\u2019 source or symbol tables. We show that <em>BinArmor </em>is able to stop real attacks\u2014including the recent noncontrol data attack on Exim. Moreover, we did not incur a single false positive in practice. On the downside, the current overhead of <em>BinArmor </em>is high\u2014although no worse than competing technologies like taint analysis that do not catch attacks on non-control data. Specifically, we measured an overhead of 70% for <span class=\"s1\">gzip</span>, 16%-180% for <span class=\"s1\">lighttpd</span>, and 190% for the <span class=\"s1\">nbench </span>suite.</p><p>\u00a0</p></div>"], "author": ["Asia Slowinska,\u00a0", "\u00a0Traian Stancescu,\u00a0", "\u00a0Herbert Bos,\u00a0"], "title": ["Body Armor for Binaries: Preventing Buffer Overflows Without Recompilation"], "affiliation": ["Vrije Universiteit Amsterdam;", "Google, Inc.;", "Vrije Universiteit Amsterdam"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final117.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Despite the increasing importance of protecting confidential data, building secure software remains as challenging <span class=\"s1\">as ever. This paper describes <em>Aeolus</em>, a new platform for building secure distributed applications. Aeolus uses in</span>formation flow control to provide confidentiality and data integrity. It differs from previous information flow control systems in a way that we believe makes it easier to understand and use. Aeolus uses a new, simpler security model, <span class=\"s2\">the first to combine a standard principal-based scheme </span><span class=\"s1\">for authority management with thread-granularity infor</span><span class=\"s2\">mation flow tracking. The principal hierarchy matches the way developers already reason about authority and </span><span class=\"s1\">access control, and the coarse-grained information flow </span>tracking eases the task of defining a program\u2019s security re<span class=\"s3\">strictions. In addition, Aeolus provides a number of new </span><span class=\"s1\">mechanisms (authority closures, compound tags, boxes, </span><span class=\"s2\">and shared volatile state) that support common design </span><span class=\"s1\">patterns in secure application design.</span></p><p>\u00a0</p></div>"], "author": ["Winnie Cheng,\u00a0", "\u00a0Dan R.K. Ports and\u00a0David Schultz,\u00a0", "\u00a0Victoria Popic,\u00a0", "\u00a0Aaron Blankstein,\u00a0", "\u00a0James Cowling and\u00a0Dorothy Curtis,\u00a0", "\u00a0Liuba Shrira,\u00a0", "\u00a0Barbara Liskov,\u00a0"], "title": ["Abstractions for Usable Information Flow Control in Aeolus"], "affiliation": ["IBM Research;", "MIT CSAIL;", "Stanford;", "Princeton;", "MIT CSAIL;", "Brandeis;", "MIT CSAIL"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final159.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Many Web applications (meaning sites that employ JavaScript) incorporate third-party code and, for reasons rooted in today\u2019s Web ecosystem, are vulnerable to bugs or malice in that code. Our goal is to give Web developers a mechanism that (a) contains included code, limiting (or eliminating) its influence as appropriate; and (b) is deployable today, or very shortly. While the goal of containment is far from new, the requirement of deployability leads us to a new design point, one that applies the OS ideas of sandboxing and virtualization to the JavaScript context. Our approach, called TreeHouse, sandboxes JavaScript code by repurposing a feature of current browsers (namely Web Workers). TreeHouse virtualizes the browser\u2019s API to the sandboxed code (allowing the code to run with few or no modifications) and gives the application author fine-grained control over that code. Our implementation and evaluation of TreeHouse show that its overhead is modest enough to handle performance-sensitive applications and that sandboxing existing code is not difficult.</p><p>\u00a0</p></div>"], "author": ["Lon Ingram,\u00a0", "\u00a0Michael Wal\ufb01sh,\u00a0"], "title": ["Treehouse: Javascript Sandboxes to Help Web Developers Help Themselves"], "affiliation": ["The University of Texas at Austin and\u00a0Waterfall Mobile;", "The University of Texas at Austin"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final165.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Current PC- and web-based applications provide insufficient security for the information they access, because vulnerabilities anywhere in a large client software stack can compromise confidentiality and integrity. We propose a new architecture for secure applications, Cloud Terminal, in which the only software running on the end host is a lightweight <em>secure thin terminal</em>, and most application logic is in a remote <em>cloud rendering engine</em>. The secure thin terminal has a very small TCB (23 KLOC) and no dependence on the untrusted OS, so it can be easily checked and remotely attested to. The terminal is also general-purpose: it simply supplies a secure display and input path to remote software. The cloud rendering engine runs an off-the-shelf application in a restricted VM hosted by the provider, but resource sharing between VMs lets one server support hundreds of users. We implement a secure thin terminal that runs on standard PC hardware and provides a responsive interface to applications like banking, email, and document editing. We also show that our cloud rendering engine can provide secure online banking for 5\u201310 cents per user per month.</p><p>\u00a0</p></div>"], "author": ["Lorenzo Martignoni,\u00a0", "\u00a0Pongsin Poosankam,\u00a0", "\u00a0Matei Zaharia,\u00a0", "\u00a0Jun Han,\u00a0", "\u00a0Stephen McCamant,\u00a0Dawn Song, and\u00a0Vern Paxson,\u00a0", "\u00a0Adrian Perrig,\u00a0", "\u00a0Scott Shenker and\u00a0Ion Stoica,\u00a0"], "title": ["Cloud Terminal: Secure Access to Sensitive Applications from Untrusted Systems"], "affiliation": ["University of California, Berkeley;", "University of California, Berkeley, and\u00a0Carnegie Mellon University;", "University of California, Berkeley;", "Carnegie Mellon University;", "University of California, Berkeley;", "Carnegie Mellon University;", "University of California, Berkeley"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final32.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Mosh (mobile shell) is a remote terminal application that supports intermittent connectivity, allows roaming, and speculatively and safely echoes user keystrokes for better interactive response over high-latency paths. Mosh is built on the State Synchronization Protocol (SSP), a new UDP-based protocol that securely synchronizes client and server state, even across changes of the client\u2019s IP address. Mosh uses SSP to synchronize a character-cell terminal emulator, maintaining terminal state at both client and server to predictively echo keystrokes. Our evaluation analyzed keystroke traces from six different users covering a period of 40 hours of real-world usage. Mosh was able to immediately display the effects of 70% of the user keystrokes. Over a commercial EV-DO (3G) network, median keystroke response latency with Mosh was less than 5 ms, compared with 503 ms for SSH. Mosh is free software, available from http://mosh.mit.edu. It was downloaded more than 15,000 times in the first week of its release.</p><p>\u00a0</p></div>"], "author": ["Keith Winstein and Hari Balakrishnan,\u00a0"], "title": ["Mosh: An Interactive Remote Shell for Mobile Clients"], "affiliation": ["M.I.T. Computer Science and Arti\ufb01cial Intelligence Laboratory"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final41_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Realizing Infrastructure-as-a-Service (IaaS) cloud requires a control platform to orchestrate cloud resource provisioning, configuration, and decommissioning across a distributed set of diverse physical resources. This orchestration is challenging due to the rapid growth of data centers, high failure rate of commodity hardware and the increasing sophistication of cloud services. This paper presents the design and implementation of T<span class=\"s1\">ROPIC</span>, a highly available, transactional resource orchestration platform for building IaaS cloud infrastructures. T<span class=\"s1\">ROPIC</span>\u2019s orchestration procedures that manipulate physical resources are transactional, automatically guaranteeing atomicity, consistency, isolation and durability of cloud operations. Through extensive evaluation of our prototype implementation, we demonstrate that T<span class=\"s1\">ROPIC </span>can meet production-scale cloud orchestration demands, while maintaining our design goals of safety, robustness, concurrency and high availability.</p><p>\u00a0</p></div>"], "author": ["Changbin Liu,\u00a0", "\u00a0Yun Mao,\u00a0Xu Chen, and\u00a0Mary F. Fern\u00e1ndez,\u00a0", "\u00a0Boon Thau Loo,\u00a0", "\u00a0Jacobus E. Van der Merwe,\u00a0"], "title": ["TROPIC: Transactional Resource Orchestration Platform in the Cloud"], "affiliation": ["University of Pennsylvania;", "AT&T Labs\u2014Research;", "University of Pennsylvania;", "AT&T Labs\u2014Research"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final236.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">YouTube traffic is bursty. These bursts trigger packet losses and stress router queues, causing TCP\u2019s congestion-control algorithm to kick in. In this pa- per, we introduce Trickle, a server-side mechanism that uses TCP to <em>rate limit </em>YouTube video streaming. Trickle paces the video stream by placing an upper bound on TCP\u2019s congestion window as a function of the streaming rate and the round-trip time. We evaluated Trickle on YouTube production data centers in Europe and India and analyzed its impact on losses, bandwidth, RTT, and video buffer under-run events. The results show that Trickle reduces the average TCP loss rate by up to 43% and the average RTT by up to 28% while maintaining the streaming rate requested by the application.</p><p>\u00a0</p></div>"], "author": ["Monia Ghobadi,\u00a0", "\u00a0Yuchung Cheng, Ankur Jain, and Matt Mathis,\u00a0"], "title": ["Trickle: Rate Limiting YouTube Video Streaming"], "affiliation": ["University of Toronto;", "Google"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final257.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Passive network monitoring applications such as intrusion detection systems are susceptible to overloads, which can be induced by traffic spikes or algorithmic singularities triggered by carefully crafted malicious packets. Under overload conditions, the system may consume all the available resources, dropping most of the monitored traffic until the overload condition is resolved. Unfortunately, such an awkward response to overloads may be easily capitalized by attackers who can intentionally overload the system to evade detection.</p> <p class=\"p1\">In this paper we propose Selective Packet Paging (SPP), a two-layer memory management design that gracefully responds to overload conditions by storing selected packets in secondary storage for later processing, while using randomization to avoid predictable evasion by sophisticated attackers. We describe the design and implementation of SPP within the widely used Libpcap packet capture library. Our evaluation shows that the detection accuracy of Snort on top of Libpcap is significantly reduced under algorithmic complexity and traffic overload attacks, while SPP makes it resistant to both algorithmic overloads and traffic bursts.</p><p>\u00a0</p></div>"], "author": ["Antonis Papadogiannakis,\u00a0", "\u00a0Michalis Polychronakis,\u00a0", "\u00a0Evangelos P. Markatos,\u00a0"], "title": ["Tolerating Overload Attacks Against Packet Capturing Systems"], "affiliation": ["FORTH-ICS;", "Columbia University;", "FORTH-ICS"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final294.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\"><span class=\"s1\">Applications do not typically view the kernel as a source of bad input. However, the kernel can behave in unusual </span>(yet permissible) ways for which applications are badly <span class=\"s2\">unprepared. We present </span><em><span class=\"s1\">Murphy</span></em><span class=\"s2\">, a language-agnostic tool </span>that helps developers discover and isolate run-time fail<span class=\"s2\">ures in their programs by simulating difficult-to-reproduce but completely-legitimate interactions between the appli</span>cation and the kernel. Murphy makes it easy to enable <span class=\"s1\">or disable sets of kernel interactions, called <em>gremlins</em>, so developers can focus on the failure scenarios that are im- </span>portant to them. Gremlins are implemented using the <span class=\"s1\">ptrace </span>interface, intercepting and potentially modify<span class=\"s2\">ing an application\u2019s system call invocation while requiring </span><span class=\"s1\">no invasive changes to the host machine.</span></p> <p class=\"p1\"><span class=\"s1\">We show how to use Murphy in a variety of modes to </span>find different classes of errors, present examples of the <span class=\"s2\">kernel interactions that are tested, and explain how to apply delta debugging techniques to isolate the code causing the failure. While our primary goal was the development </span>of a tool to assist in new software development, we suc<span class=\"s2\">cessfully demonstrate that Murphy also has the capability </span><span class=\"s1\">to find bugs in hardened, widely-deployed software.</span></p><p>\u00a0</p></div>"], "author": ["Zach Miller, Todd Tannenbaum,\u00a0and\u00a0Ben Liblit,\u00a0"], "title": ["Enforcing Murphy\u2019s Law for Advance Identification of Run-time Failures"], "affiliation": ["University of Wisconsin\u2014Madison"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final18.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Metaverses are three-dimensional virtual worlds where anyone can add and script new objects. Metaverses today, such as Second Life, are dull, lifeless, and stagnant because users can see and interact with only a tiny region around them, rather than a large and immersive world. Current metaverses impose this distance restriction on visibility and interaction in order to scale to large worlds, as the restriction avoids appreciable shared state in underlying distributed systems.</p> <p class=\"p1\">We present the design and implementation of the Sirikata metaverse server. The Sirikata server scales to support large, complex worlds, even as it allows users to see and interact with the entire world. It achieves both goals simultaneously by leveraging properties of the real world and 3D environments in its core systems, such as a novel distributed data structure for virtual object queries based on visible size. We evaluate core services in isolation as well as part of the entire system, demonstrating that these novel designs do not sacrifice performance. Applications developed by Sirikata users support our claim that removing the distance restriction enables new, compelling applications that are infeasible in today\u2019s metaverses.</p><p>\u00a0</p></div>"], "author": ["Ewen Cheslack-Postava, Tahir Azim, Behram F.T. Mistree, and Daniel Reiter Horn,\u00a0", "\u00a0Jeff Terrace,\u00a0", "\u00a0Philip Levis,\u00a0", "\u00a0Michael J. Freedman,\u00a0"], "title": ["A Scalable Server for 3D Metaverses"], "affiliation": ["Stanford University;", "Princeton University;", "Stanford University;", "Princeton University"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final118.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">This paper presents Granola, a transaction coordination <span class=\"s1\">infrastructure for building reliable distributed storage ap</span>plications. Granola provides a strong consistency model, <span class=\"s2\">while significantly reducing transaction coordination over</span>head. We introduce specific support for a new type of <span class=\"s1\"><em>independent</em> distributed transaction, which we can serial</span>ize with no locking overhead and no aborts due to write <span class=\"s2\">conflicts. Granola uses a novel timestamp-based coordina</span><span class=\"s3\">tion mechanism to order distributed transactions, offering </span><span class=\"s2\">lower latency and higher throughput than previous systems </span>that offer strong consistency.</p> <p class=\"p1\">Our experiments show that Granola has low overhead, is scalable and has high throughput. We implemented the TPC-C benchmark on Granola, and achieved 3\u00d7 the throughput of a platform using a locking approach.</p><p>\u00a0</p></div>"], "author": ["James Cowling and\u00a0Barbara Liskov,\u00a0"], "title": ["Granola: Low-Overhead Distributed Transaction Coordination"], "affiliation": ["MIT CSAIL"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final297.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Motivated by poor network connectivity from moving vehicles, we develop a new loss recovery method called opportunistic erasure coding (OEC). Unlike existing erasure coding methods, which are oblivious to the level of spare capacity along a path, OEC transmits coded packets only during instantaneous openings in a path\u2019s spare capacity. This transmission strategy ensures that coded packets provide as much protection as the level of spare capacity allows, without delaying or stealing capacity from data packets. OEC uses a novel encoding that greedily maximizes the amount of new data recovered by the receiver with each coded packet. We design and implement a system called <span class=\"s1\">PluriBus </span>that uses OEC in the vehicular context. We deploy it on two buses for two months and show that <span class=\"s1\">PluriBus </span>reduces the mean flow completion time by a factor of 4 for a realistic workload. We also show that OEC outperforms existing loss recovery methods in a range of lossy environments.</p><p>\u00a0</p></div>"], "author": ["Ratul Mahajan, Jitendra Padhye, Sharad Agarwal, and Brian Zill,\u00a0"], "title": ["High Performance Vehicular Connectivity with Opportunistic Erasure Coding"], "affiliation": ["Microsoft Research"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final64.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Recently many Internet services employ wide-area platforms to improve the end-user experience in the WAN. To maintain close control over their remote nodes, the wide-area systems require low-latency dissemination of new updates for system configurations, customer requirements, and task lists at runtime. However, we observe that existing data transfer systems focus on resource efficiency for open client populations, rather than focusing on completion latency for a known set of nodes. In examining this problem, we find that optimizing for latency produces strategies radically different from existing systems, and can dramatically reduce latency across a wide range of scenarios.</p> <p class=\"p1\">This paper presents a latency-sensitive file transfer system, Lsync that can be used as synchronization building block for wide-area systems where latency matters. Lsync performs novel node selection, scheduling, and adaptive policy switching that dynamically chooses the best strategy using information available at runtime. Our evaluation results from a PlanetLab deployment show that Lsync outperforms a wide variety of data transfer systems and achieves significantly higher synchronization ratio even under frequent file updates.</p><p>\u00a0</p></div>"], "author": ["Wonho Kim,\u00a0", "\u00a0KyoungSoo Park,\u00a0", "\u00a0Vivek S. Pai,\u00a0"], "title": ["Server-assisted Latency Management for Wide-area Distributed Systems"], "affiliation": ["Princeton University;", "KAIST;", "Princeton University"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final129.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Deduplication is a popular component of modern storage systems, with a wide variety of approaches. Unlike traditional storage systems, deduplication performance depends on data content as well as access patterns and meta-data characteristics. Most datasets that have been used to evaluate deduplication systems are either unrepresentative, or unavailable due to privacy issues, preventing easy comparison of competing algorithms. Understanding how both content and meta-data evolve is critical to the realistic evaluation of deduplication systems.</p> <p class=\"p1\">We developed a generic model of file system changes based on properties measured on terabytes of real, diverse storage systems. Our model plugs into a generic framework for emulating file system changes. Building on observations from specific environments, the model can generate an initial file system followed by ongoing modifications that emulate the distribution of duplicates and file sizes, realistic changes to existing files, and file system growth. In our experiments we were able to generate a 4TB dataset within 13 hours on a machine with a single disk drive. The relative error of emulated parameters depends on the model size but remains within 15% of real-world observations.</p><p>\u00a0</p></div>"], "author": ["Vasily Tarasov and Amar Mudrankit,\u00a0", "\u00a0Will Buik,\u00a0", "\u00a0Philip Shilane,\u00a0", "\u00a0Geoff Kuenning,\u00a0", "\u00a0Erez Zadok,\u00a0"], "title": ["Generating Realistic Datasets for Deduplication Analysis"], "affiliation": ["Stony Brook University;", "Harvey Mudd College;", "EMC Corporation;", "Harvey Mudd College;", "Stony Brook University"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final226.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Content-based page sharing is a technique often used in virtualized environments to reduce server memory requirements. Many systems have been proposed to capture the benefits of page sharing. However, there have been few analyses of page sharing in general, both considering its real-world utility and typical sources of sharing potential. We provide insight into this issue through an exploration and analysis of memory traces captured from real user machines and controlled virtual machines. First, we observe that absolute sharing levels (excluding zero pages) generally remain under 15%, contrasting with prior work that has often reported savings of 30% or more. Second, we find that sharing within individual machines often accounts for nearly all (&gt;90%) of the sharing potential within a set of machines, with inter-machine sharing contributing only a small amount. Moreover, even small differences between machines significantly reduce what little inter-machine sharing might otherwise be possible. Third, we find that OS features like address space layout randomization can further diminish sharing potential. These findings both temper expectations of real-world sharing gains and suggest that sharing efforts may be equally effective if employed within the operating system of a single machine, rather than exclusively targeting groups of virtual machines.</p><p>\u00a0</p></div>"], "author": ["Sean Barker,\u00a0", "\u00a0Timothy Wood,\u00a0", "\u00a0Prashant Shenoy and Ramesh Sitaraman,\u00a0"], "title": ["An Empirical Study of Memory Sharing in Virtual Machines"], "affiliation": ["University of Massachusetts Amherst;", "The George Washington University;", "University of Massachusetts Amherst"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final293.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">We present a large scale study of primary data deduplication and use the findings to drive the design of a new primary data deduplication system implemented in the Windows Server 2012 operating system. File data was analyzed from 15 globally distributed file servers hosting data for over 2000 users in a large multinational corporation.</p> <p class=\"p1\">The findings are used to arrive at a chunking and compression approach which maximizes deduplication savings while minimizing the generated metadata and producing a uniform chunk size distribution. Scaling of deduplication processing with data size is achieved using a RAM frugal chunk hash index and data partitioning \u2013 so that memory, CPU, and disk seek resources remain available to fulfill the primary workload of serving IO.</p> <p class=\"p1\">We present the architecture of a new primary data deduplication system and evaluate the deduplication performance and chunking aspects of the system.</p><p>\u00a0</p></div>"], "author": ["Ahmed El-Shimi, Ran Kalach, Ankit Kumar, Adi Oltean, Jin Li, and Sudipta Sengupta,\u00a0"], "title": ["Primary Data Deduplication\u2014Large Scale Study and System Design"], "affiliation": ["Microsoft Corporation"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final30.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">This paper presents the design and implementation of a complete embedded Python run-time system for the ARM Cortex-M3 microcontroller. The Owl embedded Python run-time system introduces several key innovations, including a toolchain that is capable of producing relocatable memory images that can be utilized directly by the run-time system and a novel foreign function interface that enables the efficient integration of native C code with Python.</p> <p class=\"p1\">The Owl system demonstrates that it is practical to run high-level languages on embedded microcontrollers. Instrumentation within the system has led to an overall system design that enables Python code to be executed with low memory and speed overheads. Furthermore, this paper presents an evaluation of an autonomous RC car that uses a controller written entirely in Python. This demonstrates the ease with which complex embedded software systems can be built using the Owl infrastructure.</p><p>\u00a0</p></div>"], "author": ["Thomas W. Barr,\u00a0Rebecca Smith, and\u00a0Scott Rixner,\u00a0"], "title": ["Design and Implementation of an Embedded Python Run-Time System"], "affiliation": ["Rice University"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final39.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Memory access bugs, including buffer overflows and uses of freed heap memory, remain a serious problem for programming languages like C and C++. Many memory error detectors exist, but most of them are either slow or detect a limited set of bugs, or both.</p> <p class=\"p1\">This paper presents AddressSanitizer, a new memory error detector. Our tool finds out-of-bounds accesses to heap, stack, and global objects, as well as use-after-free bugs. It employs a specialized memory allocator and code instrumentation that is simple enough to be implemented in any compiler, binary translation system, or even in hardware.</p> <p class=\"p1\">AddressSanitizer achieves efficiency without sacrificing comprehensiveness. Its average slowdown is just 73% yet it accurately detects bugs at the point of occurrence. It has found over 300 previously unknown bugs in the Chromium browser and many bugs in other software.</p><p>\u00a0</p></div>"], "author": ["Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitriy Vyukov,\u00a0"], "title": ["AddressSanitizer: A Fast Address Sanity Checker"], "affiliation": ["Google"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final70.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Persistence of in-memory data is necessary for many classes of application and systems software. We propose Software Persistent Memory (SoftPM), a new memory abstraction which allows <em>malloc </em>style allocations to be selectively made persistent with relative ease. Particularly, SoftPM\u2019s persistent <em>containers </em>implement automatic, orthogonal persistence for all in-memory data reachable from a developer-defined <em>root </em>structure. Writing new applications or adapting existing applications to use SoftPM only requires identifying such root structures within the code. We evaluated the correctness, ease of use, and performance of SoftPM using a suite of microbenchmarks and real world applications including a distributed MPI application, SQLite (an in-memory database), and memcachedb (a distributed memory cache). In all cases, SoftPM was incorporated with minimal developer effort, was able to store and recover data successfully, and provide significant performance speedup (e.g., up to 10X for memcachedb and 83% for SQLite).</p><p>\u00a0</p></div>"], "author": ["Jorge Guerra, Leonardo M\u00e1rmol, Daniel Campello, Carlos Crespo, Raju Rangaswami, and Jinpeng Wei,\u00a0"], "title": ["Software Persistent Memory"], "affiliation": ["Florida International University"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final136.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Rivet is the first fully-featured, browser-agnostic remote debugger for web applications. Using Rivet, developers can inspect and modify the state of live web pages that are running inside unmodified end-user web browsers. This allows developers to explore real application bugs in the context of the actual machines on which those bugs occur. To make an application Rivet-aware, developers simply add the Rivet JavaScript library to the client-side portion of the application. Later, when a user detects a problem with the application, the user informs Rivet; in turn, Rivet pauses the application and notifies a remote debug server that a debuggable session is available. The server can launch an interactive debugger front-end for a human developer, or use Rivet\u2019s live patching mechanism to automatically install a fix on the client or run diagnostics for offline analysis. Experiments show that Rivet imposes negligible overhead during normal application operation. At debug time, Rivet\u2019s network footprint is small, and Rivet is computationally fast enough to support non-trivial diagnostics and live patches.</p><p>\u00a0</p></div>"], "author": ["James Mickens,\u00a0"], "title": ["Rivet: Browser-agnostic Remote Debugging for Web Applications"], "affiliation": ["Microsoft Research"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final36.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Recently, various key/value stores have been proposed targeting clusters built from low-power CPUs. The typical network configuration is that the nodes in those clusters are connected using 1 Gigabit Ethernet. During the last couple of years, 10 Gigabit Ethernet has become commodity and is increasingly used within the data centers providing cloud computing services. The boost in network link speed, however, poses a challenge to the cluster nodes because filling the network link can be a CPU-intensive task. In particular for CPUs running in low-power mode, it is therefore important to spend CPU cycles used for networking as efficiently as possible. In this paper, we propose a modified Memcached architecture to leverage the one-side semantics of RDMA. We show how the modified Memcached is more CPU efficient and can serve up to 20% more GET operations than the standard Memcached implementation on low-power CPUs. While RDMA is a networking technology typically associated with specialized hardware, our solution uses soft-RDMA which runs on standard Ethernet and does not require special hardware.</p><p>\u00a0</p></div>"], "author": ["Patrick Stuedi,\u00a0Animesh Trivedi, and\u00a0Bernard Metzler,\u00a0"], "title": ["Wimpy Nodes with 10GbE: Leveraging One-Sided Operations in Soft-RDMA to Boost Memcached"], "affiliation": ["IBM Research, Zurich"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final161.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">A key concern with zero copy is that the data to be sent out might be mutated by applications. In this paper, fo-cusing specially on web-caching application, we observe that in most cases the data to be sent out is not supposed to be mutated by applications, while the metadata around it does get mutated. Based on this observation, we propose a lightweight software zero-copy mechanism that uses a twin memory allocator to allocate spaces for zero-copying data, and ensures such data is unchanged before being sent out with a lightweight data protection mech- anism. The only change required to an application is to allocate zero-copying data through a specific ZCopy memory allocator. To demonstrate the effectiveness of ZCopy, we have designed and implemented a prototype based on Linux and ported two applications with very little effort. Experiments with Memcached and Varnish shows that show that ZCopy can achieve up to 41% performance improvement over the vanilla Linux with less CPU consumption.</p><p>\u00a0</p></div>"], "author": ["Xiang Song and Jicheng Shi,\u00a0", "\u00a0Haibo Chen,\u00a0", "\u00a0Binyu Zang,\u00a0"], "title": ["Revisiting Software Zero-Copy for Web-caching Applications with Twin Memory Allocation"], "affiliation": ["Shanghai Jiao Tong University and Fudan University;", "Shanghai Jiao Tong University;", "Shanghai Jiao Tong University and Fudan University"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final57.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Enterprises with existing IT infrastructure are beginning to employ a hybrid cloud model where the enterprise uses its own private resources for the majority of its computing, but then \u201cbursts\u201d into the cloud when local resources are insufficient. However, current approaches to <em>cloud bursting</em> cannot be effectively automated because they heavily rely on system administrator knowledge to make decisions. In this paper we describe Seagull, a system designed to facilitate cloud bursting by determining which applications can be transitioned into the cloud most economically, and automating the movement process at the proper time. We further optimize the deployment of applications into the cloud using an intelligent precopying mechanism that proactively replicates virtualized applications, lowering the bursting time from hours to minutes. Our evaluation illustrates how our prototype can reduce cloud costs by more than 45% when bursting to the cloud, and the incremental cost added by precopying applications is offset by a burst time reduction of nearly 95%.</p><p>\u00a0</p></div>"], "author": ["Tian Guo and\u00a0Upendra Sharma,\u00a0", "\u00a0Timothy Wood,\u00a0", "\u00a0Sambit Sahu,\u00a0", "\u00a0Prashant Shenoy,\u00a0"], "title": ["Seagull: Intelligent Cloud Bursting for Enterprise Applications"], "affiliation": ["UMASS Amherst;", "The George Washington University;", "IBM Watson;", "UMASS Amherst"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final46.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Heterogeneous multicore processors (HMPs), consisting of cores with different performance/power characteristics, have been proposed to deliver higher energy efficiency than symmetric multicores. This paper investigates the opportunities and limitations in using HMPs to gain energy-efficiency. Unlike previous work focused on server systems, we focus on the client workloads typically seen in modern end-user devices. Further, beyond considering core power usage, we also consider the \u2018uncore\u2019 subsystem shared by all cores, which in modern platforms, is an increasingly important contributor to total SoC power. Experimental evaluations use client applications and usage scenarios seen on mobile devices and a unique testbed comprised of heterogeneous cores, with results that highlight the need for uncore-awareness and uncore scalability to maximize intended efficiency gains from heterogeneous cores.</p><p>\u00a0</p></div>"], "author": ["Vishal Gupta,\u00a0", "\u00a0Paul Brett,\u00a0David Koufaty, Dheeraj Reddy, and Scott Hahn,\u00a0", "\u00a0Karsten Schwan,\u00a0", "\u00a0Ganapati Srinivasa,\u00a0"], "title": ["The Forgotten \u2018Uncore\u2019: On the Energy-Ef\ufb01ciency of Heterogeneous Cores"], "affiliation": ["Georgia Tech;", "Intel Labs;", "Georgia Tech;", "Intel Corporation"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final158.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">On modern processors, hardware-assisted virtualization outperforms binary translation for most workloads. But hardware virtualization has a potential problem: virtualization exits are expensive. While hardware virtualization executes guest instructions at native speed, guest/VMM transitions can sap performance. Hardware designers attacked this problem both by reducing guest/VMM transition costs and by adding architectural extensions such as nested paging support to avoid exits.</p> <p class=\"p1\">This paper proposes complementary software techniques for reducing the exit frequency. In the simplest form, our VMM inspects guest code dynamically to detect back-to-back pairs of instructions that both exit. By handling a pair of instructions when the first one exits, we save 50% of the transition costs. Then, we generalize from pairs to <em>clusters </em>of instructions that may include loops and other control flow. We use a binary translator to generate, and cache, custom translations for handling exits. The analysis cost is paid once, when the translation is generated, but amortized over all future executions.</p> <p class=\"p1\">Our techniques have been fully implemented and validated in recent versions of VMware products. We show that clusters consistently reduce the number of exits for all examined workloads. When execution is dominated by exit costs, this translates into measurable runtime improvements. Most importantly, clusters enable substantial gains for nested virtual machines, delivering speedups as high as 1.52x. Intuitively, this result stems from the fact that transitions between the inner guest and VMM are extremely costly, as they are implemented in software by the outer VMM.</p><p>\u00a0</p></div>"], "author": ["Ole Agesen,\u00a0Jim Mattson,\u00a0Radu Rugina, and\u00a0Jeffrey Sheldon,\u00a0"], "title": ["Software Techniques for Avoiding Hardware Virtualization Exits"], "affiliation": ["VMware"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final171.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Understanding the energy consumption of a smartphone application is a key area of interest for end users, as well as application and system software developers. Previous work has only been able to provide limited information concerning the energy consumption of individual applications because of limited access to underlying hardware and system software. The energy consumption of a smartphone application is, therefore, often estimated with low accuracy and granularity. In this paper, we propose AppScope, an Android-based energy metering system. This system monitors application\u2019s hardware usage at the kernel level and accurately estimates energy consumption. AppScope is implemented as a kernel module and uses an event-driven monitoring method that generates low overhead and provides high accuracy. The evaluation results indicate that AppScope accurately estimates the energy consumption of Android applications expending approximately 35mW and 2.1% in power consumption and CPU utilization overhead, respectively.</p><p>\u00a0</p></div>"], "author": ["Chanmin Yoon, Dongwon Kim, Wonwoo Jung, Chulkoo Kang, and Hojung Cha,\u00a0"], "title": ["AppScope: Application Energy Metering Framework for Android Smartphone Using Kernel Activity Monitoring"], "affiliation": ["Yonsei University, Korea"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final319.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Graphics processing units (GPUs) have become a very powerful platform embracing a concept of heterogeneous many-core computing. However, application domains of GPUs are currently limited to specific systems, largely due to a lack of \u201cfirst-class\u201d GPU resource management for general-purpose multi-tasking systems.</p> <p class=\"p1\">We present Gdev, a new ecosystem of GPU resource management in the operating system (OS). It allows the user space as well as the OS itself to use GPUs as first-class computing resources. Specifically, Gdev\u2019s virtual memory manager supports data swapping for excessive memory resource demands, and also provides a shared device memory functionality that allows GPU contexts to communicate with other contexts. Gdev further provides a GPU scheduling scheme to virtualize a physical GPU into multiple logical GPUs, enhancing isolation among working sets of multi-tasking systems.</p> <p class=\"p1\">Our evaluation conducted on Linux and the NVIDIA GPU shows that the basic performance of our prototype implementation is reliable even compared to proprietary software. Further detailed experiments demonstrate that Gdev achieves a 2x speedup for an encrypted file system using the GPU in the OS. Gdev can also improve the makespan of dataflow programs by up to 49% exploiting shared device memory, while an error in the utilization of virtualized GPUs can be limited within only 7%.</p><p>\u00a0</p></div>"], "author": ["Shinpei Kato, Michael McThrow, Carlos Maltzahn, and Scott Brandt,\u00a0"], "title": ["Gdev: First-Class GPU Resource Management in the Operating System"], "affiliation": ["UC Santa Cruz"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final43.pdf", "https://www.usenix.org/system/files/conference/atc12/atc12_erratum.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">This paper describes Gnothi, a block replication system that separates data from metadata to provide efficient and available storage replication. Separating data from metadata allows Gnothi to execute disk accesses on subsets of replicas while using fully replicated metadata to ensure that requests are executed correctly and to speed up recovery of slow or failed replicas.</p> <p class=\"p1\">Performance evaluation shows that Gnothi can achieve 40-64% higher write throughput than previous work and significantly save storage space. Furthermore, while a failed replica recovers, Gnothi can provide about 100-200% higher throughput, while still retaining the same recovery time and while guaranteeing that recovery eventually completes.</p><p>\u00a0</p></div>"], "author": ["Yang Wang, Lorenzo Alvisi, and Mike Dahlin,\u00a0"], "title": ["Gnothi: Separating Data and Metadata for Efficient and Available Storage Replication"], "affiliation": ["The University of Texas at Austin"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final74.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Dynamically changing (reconfiguring) the membership of a replicated distributed system while preserving data consistency and system availability is a challenging problem. In this paper, we show that reconfiguration can be simplified by taking advantage of certain properties commonly provided by Primary/Backup systems. We describe a new reconfiguration protocol, recently implemented in Apache Zookeeper. It fully automates configuration changes and minimizes any interruption in service to clients while maintaining data consistency. By leveraging the properties already provided by Zookeeper our protocol is considerably simpler than state of the art.</p><p>\u00a0</p></div>"], "author": ["Alexander Shraer and\u00a0Benjamin Reed,\u00a0", "\u00a0Dahlia Malkhi,\u00a0", "\u00a0Flavio Junqueira,\u00a0"], "title": ["Dynamic Recon\ufb01guration of Primary/Backup Clusters"], "affiliation": ["Yahoo! Research;", "Microsoft Research;", "Yahoo! Research"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final92_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">We present Vivace, a key-value storage system for web applications that span many geographically-distributed sites. Vivace provides strong consistency and replicates data across sites for access locality and disaster tolerance. Vivace is designed to cope well with network congestion across sites, which occurs because the bandwidth across sites is smaller than within sites. To deal with congestion, Vivace relies on two novel algorithms that prioritize a small amount of critical data to avoid delays due to congestion. We evaluate Vivace to show its feasibility and effectiveness.</p><p>\u00a0</p></div>"], "author": ["Brian Cho,\u00a0", "\u00a0Marcos K. Aguilera,\u00a0"], "title": ["Surviving Congestion in Geo-Distributed Storage Systems"], "affiliation": ["University of Illinois at Urbana-Champaign;", "Microsoft Research Silicon Valley"]},
{"conference": ["USENIX ATC '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/atc12/atc12-final190.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Recent failures of production systems have highlighted the importance of tolerating faults beyond crashes. The industry has so far addressed this problem by hardening crash-tolerant systems with <em>ad hoc</em> error detection checks, potentially overlooking critical fault scenarios. We propose a generic and principled hardening technique for Arbitrary State Corruption (ASC) faults, which specifically model the effects of realistic data corruptions on distributed processes. Hardening does not require the use of trusted components or the replication of the process over multiple physical servers. We implemented a wrapper library to transparently harden distributed processes. To exercise our library and evaluate our technique, we obtained ASC-tolerant versions of Paxos, of a subset of the ZooKeeper API, and of an eventually consistent storage by implementing crash-tolerant protocols and automatically hardening them using our library. Our evaluation shows that the throughput of our ASC-hardened state machine replication outperforms its Byzantine-tolerant counterpart by up to 70%.</p><p>\u00a0</p></div>"], "author": ["Miguel Correia,\u00a0", "\u00a0Daniel G\u00f3mez Ferro,\u00a0Flavio P. Junqueira, and\u00a0Marco Sera\ufb01ni,\u00a0"], "title": ["Practical Hardening of Crash-Tolerant Systems"], "affiliation": ["IST-UTL/INESC-ID;", "Yahoo! Research"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final11_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">A crucial aspect of certain applications such as the ones pertaining to Intelligence domain or Health-care, is to manage and protect sensitive information effectively and efficiently. In this paper, we propose a tagging mechanism to track the flow of sensitive or valuable information in a provenance graph and automate the process of document classification. When provenance is initially recorded, the documents of a provenance graph are assumed to be annotated with tags representing their sensitivity or priority. We then propagate the tags appropriately on the newly generated documents using additional inference rules defined in this paper. This approach enables users to conveniently query to identify sensitive or valuable information, which can now be efficiently managed or protected once identified.</p><p>\u00a0</p></div>"], "author": ["Jyothsna Rachapalli, Murat Kantarcioglu, and Bhavani Thuraisingham,\u00a0"], "title": ["Tag-based Information Flow Analysis for Document Classi\ufb01cation in Provenance"], "affiliation": ["The University of Texas at Dallas"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final15.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Large system installations are increasingly configured using high-level, mostly-declarative languages. Often, different users contribute data that is compiled centrally and distributed to individual systems. Although the systems themselves have been developed with reliability and availability in mind, the configuration compilation process can lead to unforeseen vulnerabilities because of the lack of access control on the different components combined to build the final configuration. Even if simple change-based access controls are applied to validate changes to the final version, changes can be lost or incorrectly attributed. Based on the growing literature on provenance for database queries and other models of computation, we identify a potential application area for provenance to securing configuration languages.</p><p>\u00a0</p></div>"], "author": ["Paul Anderson and James Cheney,\u00a0"], "title": ["Toward Provenance-Based Security for Con\ufb01guration Languages"], "affiliation": ["University of Edinburgh"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final17.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Much has been written about security and provenance. Although both have their own large areas of concern, there is a very significant intersection. One is often brought to bear upon the other, in the study of the security of provenance. We discuss through a series of examples how provenance might be regarded as a security control in its own right. We argue that a risk-based approach to provenance is appropriate, and is already being used informally. A case study illustrates the applicability of this line of reasoning.</p><p>\u00a0</p></div>"], "author": ["Andrew Martin,\u00a0John Lyle, and\u00a0Cornelius Namilkuo,\u00a0"], "title": ["Provenance as a Security Control"], "affiliation": ["University of Oxford"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final23.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">A unique characteristics of provenance data is that it forms a directed acyclic graph (DAG) in accordance with the underlying causality dependencies between entities (acting users, action processes and data objects) involved in transactions. Data provenance raises at least two distinct security-related issues. One is how to control access to provenance data which we call Provenance Access control (PAC). The other is Provenance-based Access Control (PBAC) which focuses on how to utilize provenance data to control access to data objects. Both PAC and PBAC are built on a common foundation that requires security architects to define application-specific dependency path patterns of provenance data. Assigning application-specific semantics to these path patterns provides the foundation for effective security policy specification and administration. This paper elaborates on this common foundation of PAC and PBAC and identifies some of the differences in how this common foundation is applied in these two contexts.</p><p>\u00a0</p></div>"], "author": ["Dang Nguyen,\u00a0Jaehong Park, and\u00a0Ravi Sandhu,\u00a0"], "title": ["Dependency Path Patterns as the Foundation of Access Control in Provenance-aware Systems"], "affiliation": ["Institute for Cyber Security,\u00a0", "University of Texas at San Antonio"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final5.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">We present a new Python/C++ framework, BioLite, for implementing bioinformatics pipelines for Next-Generation Sequencing (NGS) data.<span> </span>BioLite tracks provenance of analyses, automates the collection and reporting of diagnostics (such as summary statistics and plots at intermediate stages), and profiles computational requirements. These diagnostics can be accessed across multiple stages of a pipeline, from other pipelines, and in HTML reports. Finally, we describe several use cases for diagnostics in our own analyses.</p><p>\u00a0</p></div>"], "author": ["Mark Howison,\u00a0Nicholas A. Sinnott-Armstrong, and\u00a0Casey W. Dunn,\u00a0"], "title": ["BioLite, a Lightweight Bioinformatics Framework with Automated Tracking of Diagnostics and Provenance"], "affiliation": ["Brown University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final9.pdf", "https://www.usenix.org/system/files/conference/tapp12/tapp12-final9-revised7-3-12.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Most provenance capture takes place inside particular tools \u2013 a workflow engine, a database, an operating system, or an application. However, most users have an existing toolset \u2013 a collection of different tools that work well for their needs and with which they are comfortable. Currently, such users have limited ability to collect provenance without disrupting their work and changing environments, which most users are hesitant to do. Even users who are willing to adopt new tools, may realize limited benefit from provenance in those tools if they do not integrate with their entire environment, which may include multiple languages and frameworks.</p> <p class=\"p1\">We present the Core Provenance Library (CPL), a portable, multi-lingual library that application programmers can easily incorporate into a variety of tools to collect and integrate provenance. Although the manual instrumentation adds extra work for application programmers, we show that in most cases, the work is minimal, and the resulting system solves several problems that plague more constrained provenance collection systems.</p><p>\u00a0</p></div>"], "author": ["Peter Macko and\u00a0Margo Seltzer,\u00a0"], "title": ["A General-Purpose Provenance Library"], "affiliation": ["Harvard University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final10.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Researchers in fields such as bioinformatics, CS, finance, and applied math have trouble managing the numerous code and data files generated by their computational experiments, comparing the results of trials executed with different parameters, and keeping up-to-date notes on what they learned from past successes and failures.</p> <p class=\"p1\">We created a Linux-based system called B<span class=\"s1\">URRITO </span>that automates aspects of this tedious experiment organization and notetaking process, thus freeing researchers to focus on more substantive work. B<span class=\"s1\">URRITO </span>automatically captures a researcher\u2019s computational activities and provides user interfaces to annotate the captured provenance with notes and then make queries such as, <em>\u201cWhich script versions and command-line parameters generated the output graph that this note refers to?\u201d</em></p><p>\u00a0</p></div>"], "author": ["Philip J. Guo,\u00a0", "\u00a0Margo Seltzer,\u00a0"], "title": ["BURRITO: Wrapping Your Lab Notebook in Computational Infrastructure"], "affiliation": ["Stanford University;", "Harvard University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final8.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">The end goal of provenance is to assist users in understanding their data: How was it created? When? By whom? How was it manipulated? In other words, provenance is a powerful tool to help users answer the question, \u201cIs this data fit for use?\u201d However, there is no one set of criteria that make data \u201cfit for use\u201d. The criteria depend on the user, the task at hand, and the current situation. In this work we describe Fitness Widgets, predefined queries over provenance graphs that users can customize to determine data fitness. We have implemented Fitness Widgets in our provenance system, PLUS.</p><p>\u00a0</p></div>"], "author": ["Adriane Chapman, M. David Allen, and Barbara Blaustein,\u00a0"], "title": ["It\u2019s About the Data: Provenance as a Tool for Assessing Data Fitness"], "affiliation": ["The MITRE Corporation"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final12.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>As has been frequently observed in the literature, there is a strong connection between a derived data item\u2019s provenance and its authoritativeness, utility, relevance, or probability. A standard way of obtaining a score for a derived tuple is by first assigning scores to the \u201cbase\u201d tuples from which it is derived \u2014 then using the semantics of the query and the score measure to derive a value for the tuple. This \u201cprovenance-enabled\u201d scoring has led to a variety of scenarios where tuples\u2019 intrinsic value is based on their provenance, independent of whatever other tuples exist in the data set.</p><p>However, there is another class of applications, revolving around sharing and recommendation, in which our goal may be to rank tuples by their \u201cimportance\u201d or the structure of their connectivity within the provenance graph. We argue that the most natural approach is to exploit the structure of a provenance graph to rank and recommend \u201cinteresting\u201d or \u201crelevant\u201d items to users, based on global and/or local provenance graph structure and random walk-based algorithms. We further argue that it is desirable to have a high-level declarative language to extract portions of the provenance graph and then apply the random walk computations. We extend the ProQL provenance query language to support a wide array of random walk algorithms in a high-level way, and identify opportunities for query optimization.</p><p>\u00a0</p></div>"], "author": ["Zachary G. Ives, Andreas Haeberlen, and Tao Feng,\u00a0", "\u00a0Wolfgang Gatterbauer,\u00a0"], "title": ["Querying Provenance for Ranking and Recommending"], "affiliation": ["University of Pennsylvania;", "Carnegie Mellon University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final14.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">There is general agreement that we need to understand provenance at various levels of granularity; however, there appears, as yet, to be no general agreement on what granularity means. It can refer both to the detail with which we can view a process or the detail with which we view the data. We describe a simple and straightforward method for imposing a hierarchical structure on a provenance graph and show how it can, if we want, be derived from the program whose execution created that graph.</p><p>\u00a0</p></div>"], "author": ["Peter Buneman, James Cheney, and Egor V. Kostylev, "], "title": ["Hierarchical Models of Provenance"], "affiliation": ["University of Edinburgh"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final20.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Since changes caused by database updates combine with the internal changes caused by database schema evolution, an integrated provenance management for data and metadata represents a key requirement for modern information systems. In this paper, we introduce the Archived Metadata and Provenance Manager (AM&amp;PM) system which addresses this requirement by (i) extending the Information Schema with the capability of representing the provenance of the schema and other metadata, (ii) providing a simple time-stamp based representation of the provenance of the actual data, and (iii) supporting powerful queries on the provenance of the data and the history of the metadata.</p><p>\u00a0</p></div>"], "author": ["Shi Gao and\u00a0Carlo Zaniolo,\u00a0"], "title": ["Provenance Management in Databases Under Schema Evolution"], "affiliation": ["University of California, Los Angeles"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final4_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Scientific experiments typically produce a plethora of files in the form of intermediate data or experimental results. As the project grows in scale, there is an increased need for tools and techniques that link together relevant experimental artifacts, especially if the files are heterogeneous and distributed across multiple locations. Current provenance and search techniques, however, fall short in efficiently retrieving experiment-related files, presumably because they are not tailored towards the common use cases of researchers. In this position paper, we propose Experiment Explorer, a lightweight and efficient approach that takes advantage of metadata to retrieve and visualize relevant experiment-related files.</p><p>\u00a0</p></div>"], "author": ["Delmar B. Davis and Hazeline U. Asuncion,\u00a0", "\u00a0Ghaleb Abdulla, "], "title": ["Experiment Explorer: Lightweight Provenance Search over Metadata"], "affiliation": ["University of Washington, Bothell;", "Lawrence Livermore National Laboratory"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final27.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p><span>Provenance, i.e., the lineage and processing history of data, has become increasingly important within scientific workflow systems. Provenance information can be used, e.g., to explain, debug, and reproduce the results of computational experiments as well as to determine the validity and quality of data products. Standard models for representing provenance information (such as OPM) largely focus on providing a minimal, common set of observables and constraints (in terms of causal and temporal relationships). For scientific workflow applications, however, the workflow itself and the corresponding (implicit) contraints on provenance relationships are often essential for interpreting and querying provenance information. In this paper, we propose Datalog as a \u201clingua franca\u201d for representing, querying, and specifying integrity constraints over provenance information, and introduce a unifying provenance model for specifying workflows, traces, and temporal constraints. We also demonstrate advantages of using Datalog together with the unified model through a number of examples.</span></p></div>"], "author": ["Saumen Dey and Sven K\u00f6hler, ", "\u00a0Shawn Bowers,\u00a0", "\u00a0Bertram Lud\u00e4scher,\u00a0"], "title": ["Datalog as a Lingua Franca for Provenance Querying and Reasoning"], "affiliation": ["UC Davis;", "Gonzaga University;", "UC Davis"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final18.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Rework occurs commonly in software development. This paper describes a simple rework example, namely the code refactoring process. We show that contextual information is central to supporting such rework, and we present an artifact provenance support approach that can help developers keep track of previous decisions to improve their effectiveness in rework.</p><p>\u00a0</p></div>"], "author": ["Xiang Zhao,\u00a0", "\u00a0Barbara Staudt Lerner,\u00a0", "\u00a0Leon J. Osterweil,\u00a0", "\u00a0Emery R. Boose and\u00a0Aaron M. Ellison,\u00a0"], "title": ["Provenance Support for Rework"], "affiliation": ["University of Massachusetts Amherst;", "Mount Holyoke College;", "University of Massachusetts Amherst;", "Harvard University"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final7.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\"><span class=\"s1\">Although provenance gained much attention, solutions to capture provenance do not meet all the requirements. For </span><span class=\"s2\">instance, most solution currently assume a closed world and are explicitly designed to capture provenance. Thus, </span>they fail in integrating the provenance concern into existing environments. Hence, we argue that provenance should be considered as cross-cutting concern that can <span class=\"s2\">easily be integrated into existing systems and aims at es</span>tablishing a universe of provenance. In this paper, we propose a solution concept, introduce different types of <span class=\"s1\">provenance systems, adequate software engineering tech</span><span class=\"s2\">niques, and report our experiences from a first prototype.</span></p><p>\u00a0</p></div>"], "author": ["Martin Sch\u00e4ler, Sandro Schulze, and\u00a0Gunter Saake,\u00a0"], "title": ["Toward Provenance Capturing as Cross-Cutting Concern"], "affiliation": ["University of Magdeburg, Germany"]},
{"conference": ["Workshop Program | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/tapp12/tapp12-final22.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Gathering data provenance at the operating system level is useful for capturing system-wide activity. However, many modern programs are complex and can perform numerous tasks concurrently. Capturing their provenance at this level, where processes are treated as single entities, may lead to the loss of useful intra-process detail. This can, in turn, produce false dependencies in the provenance graph. Using the LLVM compiler framework and SPADE provenance infrastructure, we investigate adding provenance instrumentation to allow intra-process provenance to be captured automatically. This results in a more accurate representation of the provenance relationships and eliminates some false dependencies. Since the capture of fine-grained provenance incurs increased overhead for storage and querying, we minimize the records retained by allowing users to declare aspects of interest and then automatically infer which provenance records are unnecessary and can be discarded.</p><p>\u00a0</p></div>"], "author": ["Dawood Tariq, Maisem Ali, and Ashish Gehani,\u00a0"], "title": ["Towards Automated Collection of Application-Level Data Provenance"], "affiliation": ["SRI International"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final30.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">CORFU<span class=\"s1\">\u00a0</span>organizes a cluster of flash devices as a single, shared log that can be accessed concurrently by multiple clients over the network. The CORFU shared log makes it easy to build distributed applications that require strong consistency at high speeds, such as databases, transactional key-value stores, replicated state machines, and metadata services. CORFU can be viewed as a distributed SSD, providing advantages over conventional SSDs such as distributed wear-leveling, network locality, fault tolerance, incremental scalability and geodistribution. A single CORFU instance can support up to 200K appends/sec, while reads scale linearly with cluster size. Importantly, CORFU is designed to work directly over network-attached flash devices, slashing cost, power consumption and latency by eliminating storage servers.</p><p>\u00a0</p></div>"], "author": ["Mahesh\u00a0Balakrishnan, Dahlia Malkhi, Vijayan Prabhakaran, and Ted Wobber,\u00a0", " Michael Wei, ", " John D. Davis, "], "title": [" CORFU: A Shared Log Design for Flash Clusters"], "affiliation": ["Microsoft Research Silicon Valley;", "University of\u00a0California, San Diego;", "Microsoft Research Silicon\u00a0Valley"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.</p><p>\u00a0</p></div>"], "author": ["Matei Zaharia, Mosharaf Chowdhury,\u00a0Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker, and Ion Stoica, ", "\n\u00a0\u00a0\u00a0\u00a0", "\n\u00a0\u00a0\u00a0\u00a0"], "title": ["Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing"], "affiliation": ["University of California, Berkeley"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final11_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Large companies like Facebook, Google, and Microsoft as well as a number of small and medium enterprises daily process massive amounts of data in batch jobs and in real time applications. This generates high network traffic, which is hard to support using traditional, oversubscribed, network infrastructures. To address this issue, several alternative network topologies have been proposed, aiming to increase the bandwidth available in enterprise clusters.</p> <p class=\"p1\">We observe that in many of the commonly used workloads, data is aggregated during the process and the output size is a fraction of the input size. This motivated us to explore a different point in the design space. Instead of increasing the bandwidth, we focus on decreasing the traffic by pushing aggregation from the edge into the network.</p> <p class=\"p1\">We built Camdoop, a MapReduce-like system running on CamCube, a cluster design that uses a direct-connect network topology with servers directly linked to other servers. Camdoop exploits the property that CamCube servers forward traffic, to perform in-network aggregation of data during the shuffle phase. Camdoop supports the same functions used in MapReduce and is compatible with existing MapReduce applications. We demonstrate that, in common cases, Camdoop significantly reduces the network traffic and provides high performance increase over a version of Camdoop running over a switch and against two production systems, Hadoop and Dryad/DryadLINQ.</p><p>\u00a0</p></div>"], "author": ["Paolo Costa, ", "Austin Donnelly, Antony Rowstron, and\u00a0Greg O'Shea, "], "title": ["Camdoop: Exploiting In-network Aggregation for Big Data  Applications"], "affiliation": ["Microsoft Research Cambridge and Imperial College London;\u00a0", "Microsoft Research Cambridge"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final142.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">The quest for higher data rates in WiFi is leading to the development of standards that make use of wide channels (e.g., 40MHz in 802.11n and 80MHz in 802.11ac). In this paper, we argue against this trend of using wider channels, and instead advocate that radios should communicate over multiple narrow channels for efficient and fair spectrum utilization. We propose WiFi-NC, a novel PHY-MAC design that allows radios to use WiFi over multiple narrow channels simultaneously. To enable WiFi-NC, we have developed the <em>compound radio</em>, a single wideband radio that exposes the abstraction of multiple narrow channel radios, each with independent transmission, reception and carrier sensing capabilities. The architecture of WiFi-NC makes it especially suitable for use in white spaces where free spectrum may be fragmented. Thus, we also develop a frequency band selection algorithm for WiFi-NC making it suitable for use in white spaces. WiFi-NC has been implemented on an FPGA-based software defined radio platform. Through real experiments and simulations, we demonstrate that WiFi-NC provides better efficiency and fairness in both common WiFi as well as future white space scenarios.</p><p>\u00a0</p></div>"], "author": ["Krishna Chintalapudi,\u00a0", " Bozidar Radunovic, ", "\u00a0Vlad Balan, ", " Michael Buettener, ", "\u00a0Srinivas Yerramalli,\u00a0", "\u00a0Vishnu Navda and Ramachandran Ramjee, "], "title": ["WiFi-NC : WiFi Over Narrow Channels"], "affiliation": ["Microsoft Research India;", "Microsoft Research\u00a0UK;", "USC;", "University of Washington;", "USC;", "Microsoft\u00a0Research India"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final146.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">We present <span class=\"s1\">WiFiNet\u00a0</span>\u2014 a system to detect, localize, and quantify the interference impact of various non-WiFi interference sources on WiFi traffic using commodity WiFi hardware alone. While there are numerous specialized solutions today that can <em>detect</em> the presence of non-WiFi devices in the unlicensed spectrum, the unique aspects of <span class=\"s1\">WiFiNet </span>are four-fold: First, <span class=\"s1\">WiFiNet </span>quantifies the actual interference impact of each non-WiFi device on specific WLAN traffic in real-time, which can vary from being a <em>whale</em> \u2014 a device that currently causes a significant reduction in WiFi throughput \u2014 to being a <em>minnow</em> \u2014 a device that currently has minimal impact. <span class=\"s1\">WiFiNet </span>continuously monitors changes in a device\u2019s impact that depend on many spatio-temporal factors. Second, it can accurately discern an individual device\u2019s impact in presence of multiple and simultaneously operating non-WiFi devices, even if the devices are of the exact same type. Third, it can pin-point the location of these non-WiFi in- terference sources in the physical space. Finally, and most importantly, <span class=\"s1\">WiFiNet </span>meets all these objectives not by using sophisticated and high resolution spectrum sensors, but by using emerging off-the-shelf WiFi cards that provide coarse-grained energy samples per sub-carrier. Our deployment and evaluation of <span class=\"s1\">WiFiNet </span>demonstrates its high accuracy \u2014 interference estimates are within \u00b110% of the ground truth and the median localization error is \u2264 4 meters. We believe a system such as <span class=\"s1\">WiFiNet </span>can empower existing WiFi clients and APs to adapt against non-WiFi interference in ways that have not been possible before.</p><p>\u00a0</p></div>"], "author": ["Shravan Rayanchu, Ashish Patro,\u00a0and Suman Banerjee, "], "title": ["Catching Whales and Minnows Using WiFiNet: Deconstructing Non-WiFi  Interference Using WiFi Hardware"], "affiliation": ["University of Wisconsin Madison"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final6.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">We revisit the design of redundancy-based loss protec<span class=\"s1\">tion schemes in light of recent advances in content-aware networking. Content-aware networks minimizes the over</span><span class=\"s2\">head of redundancy, if the redundancy is introduced in a </span>way that the network can understand. With this insight, we propose a new loss protection scheme called redun<span class=\"s2\">dant packet transmission (RPT). Using redundant video </span><span class=\"s1\">streaming as an example, we show that our approach, un</span>like FEC in traditional networks, provides low latency with high robustness and is insensitive to parameter selection. We tackle practical issues such as minimizing the impact on other traffic and the network. We show that RPT provides a simple and general mechanism for <span class=\"s2\">application-specific control and flow prioritization.</span></p><p>\u00a0</p></div>"], "author": ["Dongsu Han, ", " Ashok Anand and Aditya Akella, ", " Srinivasan Seshan, "], "title": ["RPT: Re-architecting Loss Protection for Content-Aware Networks"], "affiliation": ["Carnegie Mellon University;", "University of Wisconsin\u2014Madison;", "Carnegie Mellon University"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final33.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Internet services run on multiple servers in different locations, serving clients that are often mobile and multi-homed. This does not match well with today\u2019s network stack, designed for communication between fixed hosts with topology-dependent addresses. As a result, on-l<span class=\"s1\">ine service providers resort to clumsy and management-intensive work-arounds\u2014forfeiting the scalability of hierarchical addressing to support virtual server migration, </span><span class=\"s2\">directing all client traffic through dedicated load balancers, </span><span class=\"s1\">restarting connections when hosts move, and so on.</span></p> <p class=\"p1\"><span class=\"s2\">In this paper, we revisit the design of the network stack </span><span class=\"s1\">to meet the needs of online services. The centerpiece of </span>our Serval architecture is a new Service Access Layer (SAL) that sits above an unmodified network layer, and <span class=\"s1\">enables applications to communicate directly on service </span>names. The SAL provides a clean <em>service-level</em> con<span class=\"s2\">trol/data plane split, enabling policy, control, and in-stack </span><span class=\"s1\">name-based routing that connects clients to services via </span>diverse discovery techniques. By tying <em>active sockets</em> to the control plane, applications trigger updates to service routing state upon invoking socket calls, ensuring up-to-date service resolution. With Serval, end-points <span class=\"s1\">can seamlessly change network addresses, migrate flows </span>across interfaces, or establish additional flows for effi<span class=\"s2\">cient and uninterrupted service access. Experiments with </span>our high-performance in-kernel prototype, and several <span class=\"s1\">example applications, demonstrate the value of a unified networking solution for online services.</span></p><p>\u00a0</p></div>"], "author": ["Erik Nordstr\u00f6m, David Shue, Prem Gopalan, Rob Kiefer, and Matvey Arye, ", " Steven Ko, ", " Jennifer Rexford and Michael J. Freedman, ", " \u00a0\u00a0\u00a0\u00a0"], "title": ["Serval: An End-Host Stack for Service-Centric Networking"], "affiliation": ["Princeton University;", "University of Buffalo;", "Princeton University"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final218.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Content distribution networks (CDNs) have started to adopt <em>hybrid</em> designs, which employ both dedicated edge servers and resources contributed by clients. Hybrid designs combine many of the advantages of infrastructure- based and peer-to-peer systems, but they also present new challenges. This paper identifies <em>reliable client accounting</em> as one such challenge. Operators of hybrid CDNs are accountable to their customers (i.e., content providers) for the CDN\u2019s performance. Therefore, they need to offer reliable quality of service and a detailed account of content served. Service quality and accurate accounting, however, depend in part on interactions among untrusted clients. Using the Akamai NetSession client network in a case study, we demonstrate that a small number of malicious clients used in a clever attack could cause significant accounting inaccuracies.</p> <p class=\"p1\">We present a method for providing reliable accounting of client interactions in hybrid CDNs. The proposed method leverages the unique characteristics of hybrid systems to limit the loss of accounting accuracy and service quality caused by faulty or compromised clients. We also describe RCA, a system that applies this method to a commercial hybrid content-distribution network. Using trace-driven simulations, we show that RCA can detect and mitigate a variety of attacks, at the expense of a moderate increase in logging overhead.</p><p>\u00a0</p></div>"], "author": ["Paarijaat Aditya, ", " Mingchen Zhao, ", " Yin Lin, ", " Andreas Haeberlen, ", " Peter Druschel, ", " Bruce Maggs, ", " Bill Wishon, "], "title": ["Reliable Client Accounting for P2P-Infrastructure Hybrids"], "affiliation": ["Max Planck Institute for Software Systems (MPI-SWS);", "University of Pennsylvania;", "Duke University and ", ";", "University of Pennsylvania;", "Max Planck Institute for Software Systems (MPI-SWS);", "Duke University and ", ";", "Akamai Technologies"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final8.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Today\u2019s networks typically carry or deploy dozens of protocols and mechanisms simultaneously such as MPLS, NAT, ACLs and route redistribution. Even when individual protocols function correctly, failures can arise from the complex interactions of their aggregate, requiring network administrators to be masters of detail. Our goal is to automatically find an important class of failures, regardless of the protocols running, for both operational and experimental networks.</p> <p class=\"p1\">To this end we developed a general and protocol-agnostic framework, called <em>Header Space Analysis</em> (HSA). Our formalism allows us to statically check network specifications and configurations to identify an important class of failures such as <em>Reachability</em> Failures, <em>Forwarding Loops</em> and <em>Traffic Isolation and Leakage</em> problems. In HSA, protocol header fields are not first class entities; instead we look at the entire packet header as a concatenation of bits without any associated meaning. Each packet is a point in the {0, 1}<em>^L\u00a0</em>space where <em>L</em> is the maximum length of a packet header, and networking boxes transform packets from one point in the space to another point or set of points (multicast).</p> <p class=\"p1\">We created a library of tools, called Hassel, to implement our framework, and used it to analyze a variety of networks and protocols. Hassel was used to analyze the Stanford University backbone network, and found all the forwarding loops in less than 10 minutes, and verified reachability constraints between two subnets in 13 seconds. It also found a large and complex loop in an experimental loose source routing protocol in 4 minutes.</p><p>\u00a0</p></div>"], "author": ["Peyman Kazemian, ", " George Varghese, ", " Nick McKeown, "], "title": ["Header Space Analysis: Static Checking for Networks"], "affiliation": ["Stanford University;", "UCSD and Yahoo! Research;", "Stanford University"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final105.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">The emergence of OpenFlow-capable switches enables exciting new network functionality, at the risk of programming errors that make communication less reliable. The centralized programming model, where a single controller program manages the network, seems to reduce the likelihood of bugs. However, the system is inherently distributed and asynchronous, with events happening at different switches and end hosts, and inevitable delays affecting communication with the controller. In this paper, we present efficient, systematic techniques for testing unmodified controller programs. Our NICE tool applies model checking to explore the state space of the entire system\u2014the controller, the switches, and the hosts. Scalability is the main challenge, given the diversity of data packets, the large system state, and the many possible event orderings. To address this, we propose a novel way to augment model checking with symbolic execution of event handlers (to identify representative packets that exercise code paths on the controller). We also present a simplified OpenFlow switch model (to reduce the state space), and effective strategies for generating event interleavings likely to uncover bugs. Our prototype tests Python applications on the popular NOX platform. In testing three real applications\u2014a MAC-learning switch, in-network server load balancing, and energy-efficient traffic engineering\u2014we uncover eleven bugs.</p><p>\u00a0</p></div>"], "author": ["Marco Canini, Daniele Venzano, Peter Pere\u0161\u00edni, and Dejan Kosti\u0107, ", " Jennifer Rexford, "], "title": ["A NICE Way to Test OpenFlow Applications"], "affiliation": ["EPFL;", "Princeton University"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/hot-ice12/nsdi12-final143_0.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">To become a credible alternative to specialized hardware, general-purpose networking needs to offer not only flexibility, but also predictable performance. Recent projects have demonstrated that general-purpose multicore hardware is capable of high-performance packet processing, but under a crucial simplifying assumption of uniformity: all processing cores see the same type/amount of traffic and run identical code, while all packets incur the same type of conventional processing (e.g., IP forwarding). Instead, we present a general-purpose packet-processing system that combines ease of programmability with predictable performance, while running a diverse set of applications and serving multiple clients with different needs. Offering predictability in this context is considered a hard problem, because software processes contend for shared hardware resources\u2014caches, memory controllers, buses\u2014in unpredictable ways. Still, we show that, in our system, (a) the way in which resource contention affects performance is predictable and (b) the overall performance depends little on how different processes are scheduled on different cores. To the best of our knowledge, our results constitute the first evidence that, when designing software network equipment, flexibility and predictability are not mutually exclusive goals.</p><p>\u00a0</p></div>"], "author": ["Mihai Dobrescu and Katerina Argyraki, ", " Sylvia Ratnasamy, "], "title": ["Toward Predictable Performance in Software Packet-Processing Platforms"], "affiliation": ["EPFL, Switzerland;", "UC Berkeley"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final17.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\"><span class=\"s1\">While third-party tracking on the web has garnered much </span>attention, its workings remain poorly understood. Our <span class=\"s1\">goal is to dissect how mainstream web tracking occurs in </span>the wild. We develop a client-side method for detecting and classifying five kinds of third-party trackers based on how they manipulate browser state. We run our detection system while browsing the web and observe a rich ecosystem, with over 500 unique trackers in our measurements alone. We find that most commercial <span class=\"s1\">pages are tracked by multiple parties, trackers vary widely </span>in their coverage with a small number being widely deployed, and many trackers exhibit a combination of tracking behaviors. Based on web search traces taken <span class=\"s1\">from AOL data, we estimate that several trackers can each capture more than 20% of a user\u2019s browsing behavior. We further assess the impact of defenses on tracking and find that no existing browser mechanisms prevent tracking by </span><span class=\"s2\">social media sites via widgets while still allowing those </span>widgets to achieve their utility goals, which leads us to <span class=\"s1\">develop a new defense. To the best of our knowledge, our </span><span class=\"s3\">work is the most complete study of web tracking to date.</span></p><p>\u00a0</p></div>"], "author": ["Franziska Roesner, Tadayoshi Kohno, and David Wetherall, "], "title": ["Detecting and Defending Against Third-Party Tracking on the Web"], "affiliation": ["University of Washington"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final24.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">To maintain the privacy of individual users\u2019 personal data, a growing number of researchers propose storing user data in client computers or personal data stores in the cloud, and allowing users to tightly control the release of that data. While this allows specific applications to use certain approved user data, it precludes broad statistical analysis of user data. Distributed differential privacy is one approach to enabling this analysis, but previous proposals are not practical in that they scale poorly, or that they require trusted clients. This paper proposes a design that overcomes these limitations. It places tight bounds on the extent to which malicious clients can distort answers, scales well, and tolerates churn among clients. This paper presents a detailed design and analysis, and gives performance results of a complete implementation based on the deployment of over 600 clients.</p><p>\u00a0</p></div>"], "author": ["Ruichuan Chen, Alexey Reznichenko, and Paul Francis,\u00a0", " Johannes Gehrke, "], "title": ["Towards Statistical Queries over Distributed Private User Data"], "affiliation": ["Max Planck Institute for Software Systems (MPI-SWS);", "Cornell University"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final118.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">With mobile phones becoming first-class citizens in the online world, the rich location data they bring to the table is set to revolutionize all aspects of online life including content delivery, recommendation systems, and advertising. However, user-tracking is a concern with such location-based services, not only because location data can be linked uniquely to individuals, but because the low-level nature of current location APIs and the resulting dependence on the cloud to synthesize useful representations virtually guarantees such tracking.</p> <p class=\"p1\">In this paper, we propose <em>privacy-preserving location-based matching </em>as a fundamental platform primitive and as an alternative to exposing low-level, latitude-longitude (lat-long) coordinates to applications. Applications set rich location-based triggers and have these be fired based on location updates either from the local device or from a remote device (e.g., a friend\u2019s phone). Our Koi platform, comprising a privacy-preserving matching service in the cloud and a phone-based agent, realizes this primitive across multiple phone and browser platforms. By mask-ing low-level lat-long information from applications, Koi not only avoids leaking privacy-sensitive information, it also eases the task of programmers by providing a higher-level abstraction that is easier for applications to build upon. Koi\u2019s privacy-preserving protocol prevents the cloud service from tracking users. We verify the non-tracking properties of Koi using a theorem prover, illustrate how privacy guarantees can easily be added to a wide range of location-based applications, and show that our public deployment is performant, being able to perform 12K matches per second on a single core.</p><p>\u00a0</p></div>"], "author": ["Saikat Guha, Mudit Jain, and Venkata N. Padmanabhan, "], "title": ["Koi: A Location-Privacy Platform for Smartphone Apps"], "affiliation": ["Microsoft Research India"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final42_2.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Users increasingly rely on the trustworthiness of the information exposed on Online Social Networks (OSNs). In addition, OSN providers base their business models on the marketability of this information. However, OSNs suffer from abuse in the form of the creation of fake accounts, which do not correspond to real humans. Fakes can introduce spam, manipulate online rating, or exploit knowledge extracted from the network. OSN operators currently expend significant resources to detect, manually verify, and shut down fake accounts. Tuenti, the largest OSN in Spain, dedicates 14 full-time employees in that task alone, incurring a significant monetary cost. Such a task has yet to be successfully automated because of the difficulty in reliably capturing the diverse behavior of fake and real OSN profiles.</p> <p class=\"p1\">We introduce a new tool in the hands of OSN operators, which we call <em>SybilRank</em>. It relies on social graph properties to rank users according to their perceived likelihood of being fake (Sybils). SybilRank is computationally efficient and can scale to graphs with hundreds of millions of nodes, as demonstrated by our Hadoop prototype. We deployed SybilRank in Tuenti\u2019s operation center. We found that <span class=\"s1\">\u223c</span>90% of the 200K accounts that SybilRank designated as most likely to be fake, actually warranted suspension. On the other hand, with Tuenti\u2019s current user-report-based approach only <span class=\"s1\">\u223c</span>5% of the inspected accounts are indeed fake.</p><p>\u00a0</p></div>"], "author": ["Qiang Cao, ", " Michael Sirivianos, ", " Xiaowei Yang, ", " Tiago Pregueiro, "], "title": ["Aiding the Detection of Fake Accounts in Large Scale Social Online Services"], "affiliation": ["Duke University;", "Telefonica Research;", "Duke University;", "Tuenti, Telefonica Digital"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final79.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Large enterprises can save significant energy and money by putting idle desktop machines to sleep. Many systems that let desktops sleep and wake them on demand have been proposed, but enterprise IT departments refuse to deploy them because they require special hardware, disruptive virtualization technology, or dedicated per-subnet proxies, none of which are cost-effective. In response, we devised GreenUp, a minimal software-only system that allows any machine to act as a proxy for other sleeping machines in its subnet. To achieve this, GreenUp uses novel distributed techniques that spread load through randomization, efficiently synchronize state within a subnet, and maintain a minimum number of proxies despite the potential for correlated sleep times. In this paper, we present the details of GreenUp\u2019s design as well as a theoretical analysis demonstrating its correctness and efficiency, using empirically-derived models where appropriate. We also present results and lessons from a seven-month live deployment on over 100 machines; a larger deployment on ~1,100 machines is currently ongoing.</p><p>\u00a0</p></div>"], "author": ["Siddhartha Sen, ", " Jacob R. Lorch, Richard Hughes, Carlos Garcia Jurado Suarez, and Brian Zill, ", " Weverton Cordeiro, ", "Jitendra Padhye,\u00a0"], "title": ["Don't Lose Sleep Over Availability: The GreenUp Decentralized Wakeup Service"], "affiliation": ["Princeton University;", "Microsoft Research;", "Universidade Federal do Rio Grande do Sul;\u00a0", "Microsoft Research"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final82.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Industry experience indicates that the ability to incrementally expand data centers is essential. However, existing high-bandwidth network designs have rigid structure that interferes with incremental expansion. We present Jellyfish, a high-capacity network interconnect which, by adopting a random graph topology, yields itself naturally to incremental expansion. Somewhat surprisingly, Jellyfish is more cost-efficient than a fat-tree, supporting as many as 25% more servers at full capacity using the same equipment at the scale of a few thousand nodes, and this advantage improves with scale. Jellyfish also allows great flexibility in building networks with different degrees of oversubscription. However, Jellyfish\u2019s unstructured design brings new challenges in routing, physical layout, and wiring. We describe approaches to resolve these challenges, and our evaluation suggests that Jellyfish could be deployed in today\u2019s data centers.</p><p>\u00a0</p></div>"], "author": ["Ankit Singla and Chi-Yao Hong, ", " Lucian Popa, ", "\u00a0P. Brighten Godfrey, "], "title": ["Jellyfish: Networking Data Centers Randomly"], "affiliation": ["University of Illinois at Urbana-Champaign;", "HP Labs; ", "University of Illinois at Urbana-Champaign"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final88.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Data center networks (DCNs) form the backbone infrastructure of many large-scale enterprise applications as well as emerging cloud computing providers. This paper describes the design, implementation and evaluation of OSA, a novel Optical Switching Architecture for DCNs. Leveraging runtime reconfigurable optical devices, OSA dynamically changes its topology and link capacities, thereby achieving unprecedented flexibility to adapt to dynamic traffic patterns. Extensive analytical simulations using both real and synthetic traffic patterns demonstrate that OSA can deliver high bisection bandwidth (60%-100% of the non-blocking architecture). Implementation and evaluation of a small-scale functional prototype further demonstrate the feasibility of OSA.</p><p>\u00a0</p></div>"], "author": ["Kai Chen, ", " Ankit Singla, ", " Atul Singh, Kishore Ramachandran, Lei Xu, and Yueping Zhang, ", " Xitao Wen and Yan Chen, "], "title": ["OSA: An Optical Switching Architecture for Data Center Networks with Unprecedented Flexibility"], "affiliation": ["Northwestern University;", "UIUC;", "NEC Labs America, Inc.;", "Northwestern University"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/pacman.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Data-intensive analytics on large clusters is important for modern Internet services. As machines in these clusters have large memories, in-memory caching of inputs is an effective way to speed up these analytics jobs. The key challenge, however, is that these jobs run multiple tasks in parallel and a job is sped up only when inputs of all such parallel tasks are cached. Indeed, a single task whose input is not cached can slow down the entire job. To meet this \u201call-or-nothing\u201d property, we have built <span class=\"s1\">PACMan</span>, a caching service that coordinates access to the distributed caches. This coordination is essential to improve job completion times and cluster efficiency. To this end, we have implemented two cache replacement policies on top of <span class=\"s1\">PACMan</span>\u2019s coordinated infrastructure \u2014 LIFE that minimizes average completion time by evicting large incomplete inputs, and LFU-F that maximizes cluster efficiency by evicting less frequently accessed inputs. Evaluations on production workloads from Facebook and Microsoft Bing show that <span class=\"s1\">PACMan </span>reduces average completion time of jobs by 53% and 51% (small interactive jobs improve by 77%), and improves efficiency of the cluster by 47% and 54%, respectively.</p><p>\u00a0</p></div>"], "author": ["Ganesh Ananthanarayanan, Ali Ghodsi, and Andrew Wang, ", " Dhruba Borthakur, ", " Srikanth Kandula, ", " Scott Shenker and Ion Stoica, "], "title": ["PACMan: Coordinated Memory Caching for Parallel Jobs"], "affiliation": ["University of California, Berkeley;", "Facebook;", "Microsoft Research;", "University of California, Berkeley"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final228.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Performant execution of data-parallel jobs needs good execution plans. Certain properties of the code, the data, and the interaction between them are crucial to generate these plans. Yet, these properties are difficult to estimate due to the highly distributed nature of these frameworks, the freedom that allows users to specify arbitrary code as operations on the data, and since jobs in modern clusters have evolved beyond single map and reduce phases to logical graphs of operations. Using fixed apriori estimates of these properties to choose execution plans, as modern systems do, leads to poor performance in several instances. We present RoPE, a first step towards re-optimizing data-parallel jobs. RoPE collects certain code and data properties by piggybacking on job execution. It adapts execution plans by feeding these properties to a query optimizer. We show how this improves the future invocations of the same (and similar) jobs and characterize the scenarios of benefit. Experiments on Bing\u2019s production clusters show up to 2\u00d7 improvement across response time for production jobs at the <em>75th</em> percentile while using 1.5\u00d7 fewer resources.</p></div>"], "author": ["Sameer Agarwal, ", " Srikanth Kandula, ", " Nico Bruno and Ming-Chuan Wu, ", " Ion Stoica, ", " Jingren Zhou, "], "title": ["Reoptimizing Data Parallel Computing"], "affiliation": ["University of California, Berkeley;", "Microsoft Research;", "Microsoft Bing;", "University of California, Berkeley;", "Microsoft Bing"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final98.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Map/Reduce style data-parallel computation is characterized by the extensive use of user-defined functions for data processing and relies on data-shuffling stages to prepare data partitions for parallel computation. Instead of treating user-defined functions as \u201cblack boxes\u201d, we propose to analyze those functions to turn them into \u201cgray boxes\u201d that expose opportunities to optimize data shuffling. We identify useful functional properties for user-defined functions, and propose S<span class=\"s1\">UDO</span>, an optimization framework that reasons about data-partition properties, functional properties, and data shuffling. We have assessed this optimization opportunity on over 10,000 data-parallel programs used in production SCOPE clusters, and designed a framework that is incorporated it into the production system. Experiments with real SCOPE programs on real production data have shown that this optimization can save up to 47% in terms of disk and network I/O for shuffling, and up to 48% in terms of cross-pod network traffic.</p><p>\u00a0</p></div>"], "author": ["Jiaxing Zhang and Hucheng Zhou, ", " Rishan Chen, ", " Xuepeng Fan,\u00a0", "\u00a0Zhenyu Guo and Haoxiang Lin, ", " Jack Y. Li, ", " Wei Lin and Jingren Zhou, ", " Lidong Zhou, "], "title": ["Optimizing Data Shuffling in Data-Parallel Computation by Understanding User-Defined Functions"], "affiliation": ["Microsoft Research Asia;", "Microsoft Research Asia and\u00a0", "Peking University;", "Microsoft Research Asia and Huazhong University of Science and Technology;", "Microsoft Research Asia;", "Microsoft Research Asia and\u00a0", "Georgia Institute of Technology;", "Microsoft Bing;", "Microsoft Research Asia"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final96.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Network deployments handle changing application, workload, and policy requirements via the deployment of specialized network appliances or \u201cmiddleboxes\u201d. Today, however, middlebox platforms are expensive and closed systems, with little or no hooks for extensibility. Furthermore, they are acquired from independent vendors and deployed as standalone devices with little cohesiveness in how the ensemble of middleboxes is managed. As network requirements continue to grow in both scale and variety, this bottom-up approach puts middlebox deployments on a trajectory of growing device sprawl with corresponding escalation in capital and management costs.</p> <p class=\"p1\">To address this challenge, we present CoMb, a new architecture for middlebox deployments that systematically explores opportunities for <em>consolidation</em>, both at the level of building individual middleboxes and in managing a network of middleboxes. This paper addresses key resource management and implementation challenges that arise in exploiting the benefits of consolidation in middlebox deployments. Using a prototype implementation in Click, we show that CoMb reduces the network provisioning cost 1.8\u20132.5\u00d7 and reduces the load imbalance in a network by 2\u201325\u00d7.</p><p>\u00a0</p></div>"], "author": ["Vyas Sekar, ", " Norbert Egi, ", " Sylvia Ratnasamy, ", " Michael K. Reiter, ", " Guangyu Shi, "], "title": ["Design and Implementation of a Consolidated Middlebox Architecture"], "affiliation": ["Intel Labs;", "Huawei;", "UC Berkeley;", "UNC Chapel Hill;", "Huawei"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final149.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Network devices for the home such as remotely controllable locks, lights, thermostats, cameras, and motion sensors are now readily available and inexpensive. In theory, this enables scenarios like remotely monitoring cameras from a smartphone or customizing climate control based on occupancy patterns. However, in practice today, such smarthome scenarios are limited to expert hobbyists and the rich because of the high overhead of managing and extending current technology.</p> <p class=\"p1\">We present <span class=\"s1\">HomeOS</span>, a platform that bridges this gap by presenting users and developers with a PC-like abstraction for technology in the home. It presents network devices as peripherals with abstract interfaces, enables cross-device tasks via applications written against these interfaces, and gives users a management interface designed for the home environment. <span class=\"s1\">HomeOS </span>already has tens of applications and supports a wide range of devices. It has been running in 12 real homes for 4\u20138 months, and 42 students have built new applications and added support for additional devices independent of our efforts.</p><p>\u00a0</p></div>"], "author": ["Colin Dixon, ", " Ratul Mahajan, Sharad Agarwal, A.J. Brush, Bongshin Lee, Stefan Saroiu, and Paramvir Bahl, "], "title": ["An Operating System for the Home"], "affiliation": ["IBM Research;", "Microsoft Research"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final61.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Diagnosis and correction of performance issues in modern, large-scale distributed systems can be a daunting task, since a single developer is unlikely to be familiar with the entire system and it is hard to characterize the behavior of a software system without completely understanding its internal components. This paper describes D<small>ISTALYZER</small>, an automated tool to support developer investigation of performance issues in distributed systems. We aim to leverage the vast log data available from large scale systems, while reducing the level of knowledge required for a developer to use our tool. Specifically, given two sets of logs, one with good and one with bad performance, D<span class=\"s1\">ISTALYZER </span>uses <em>machine learning </em>techniques to compare system behaviors extracted from the logs and automatically infer the strongest associations between system components and performance. The tool outputs a set of inter-related event occurrences and variable values that exhibit the largest divergence across the logs sets and most directly affect the overall performance of the system. These patterns are presented to the developer for inspection, to help them understand which system component(s) likely contain the <em>root cause </em>of the observed performance issue, thus alleviating the need for many human hours of manual inspection. We demonstrate the generality and effectiveness of D<span class=\"s1\">ISTALYZER </span>on three real distributed systems by showing how it discovers and highlights the root cause of six performance issues across the systems. D<span class=\"s1\">ISTALYZER </span>has broad applicability to other systems since it is dependent only on the logs for input, and not on the source code.</p><p>\u00a0</p></div>"], "author": ["Karthik Nagaraj, Charles Killian, and Jennifer Neville, "], "title": ["Structured Comparative Analysis of Systems Logs to Diagnose Performance Problems"], "affiliation": ["Purdue University"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final112.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">When organizations move computation to the cloud, they must choose from a myriad of cloud services that can be used to outsource these jobs. The impact of this choice on price and performance is unclear, even for technical users. To further complicate this choice, factors like price fluctuations due to spot markets, or the cost of recovering from faults must also be factored in. In this paper, we present Conductor, a system that frees cloud customers from the burden of deciding which services to use when deploying MapReduce computations in the cloud. With Conductor, customers only specify goals, e.g., minimizing monetary cost or completion time, and the system automatically selects the best cloud services to use, deploys the computation according to that selection, and adapts to changing conditions at deployment time. The design of Conductor includes several novel features, such as a system to manage the deployment of cloud computations across different services, and a resource abstraction layer that provides a unified interface to these services, therefore hiding their low-level differences and simplifying the planning and deployment of the computation. We implemented Conductor and integrated it with the Hadoop framework. Our evaluation using Amazon Web Services shows that Conductor can find very subtle opportunities for cost savings while meeting deadline requirements, and that Conductor incurs a modest overhead due to planning computations and the resource abstraction layer.</p><p>\u00a0</p></div>"], "author": ["Alexander Wieder, Pramod Bhatotia, Ansley Post, and Rodrigo Rodrigues, "], "title": ["Orchestrating the Deployment of Computations in the Cloud with Conductor"], "affiliation": ["Max Planck Institute for Software Systems (MPI-SWS)"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final67.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Internet applications increasingly employ TCP not as a <em>stream abstraction</em>, but as a <em>substrate</em> for application-level transports, a use that converts TCP\u2019s in-order semantics from a convenience blessing to a performance curse. As Internet evolution makes TCP\u2019s use as a substrate likely to grow, we offer <em>Minion</em>, an architecture for backward-compatible out-of-order delivery atop TCP and TLS. Small OS API extensions allow applications to manage TCP\u2019s send buffer and to receive TCP segments out-of-order. Atop these extensions, Minion builds application-level protocols offering true unordered datagram delivery, within streams preserving strict wire-compatibility with unsecured or TLS-secured TCP connections. Minion\u2019s protocols can run on unmodified TCP stacks, but benefit incrementally when either endpoint is upgraded, for a backward-compatible deployment path. Experiments suggest that Minion can noticeably improve performance of applications such as conferencing, virtual private networking, and web browsing, while incurring minimal CPU or bandwidth costs.</p><p>\u00a0</p></div>"], "author": ["Michael F. Nowlan,\u00a0", "Nabin Tiwari and\u00a0Janardhan Iyengar, ", " Syed Obaid Amin and\u00a0Bryan Ford, "], "title": ["Fitting Square Pegs Through Round Pipes: Unordered Delivery Wire-Compatible with TCP and TLS"], "affiliation": ["Yale University;\u00a0", "Franklin and Marshall College;", "Yale University"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final125.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">Networks have become multipath: mobile devices have multiple radio interfaces, datacenters have redundant paths and multihoming is the norm for big server farms. Mean- while, TCP is still only single-path.</p> <p class=\"p1\">Is it possible to extend TCP to enable it to support multiple paths for current applications on today\u2019s Internet? The answer is positive. We carefully review the constraints\u2014partly due to various types of middleboxes\u2014 that influenced the design of Multipath TCP and show how we handled them to achieve its deployability goals.</p> <p class=\"p1\">We report our experience in implementing Multipath TCP in the Linux kernel and we evaluate its performance. Our measurements focus on the algorithms needed to efficiently use paths with different characteristics, notably send and receive buffer tuning and segment reordering. We also compare the performance of our implementation with regular TCP on web servers. Finally, we discuss the lessons learned from designing MPTCP.</p><p>\u00a0</p></div>"], "author": ["Costin Raiciu, ", " Christoph Paasch and Sebastien Barre, ", " Alan Ford;\u00a0Michio Honda, ", " Fabien Duchene and Olivier Bonaventure, ", " Mark Handley, ", "\u00a0\u00a0\u00a0\u00a0"], "title": ["How Hard Can It Be? Designing and Implementing a Deployable Multipath TCP"], "affiliation": ["Universitatea Politehnica Bucuresti;", "Universit\u00e9 Catholique de Louvain;", "Keio University;", "Universit\u00e9 Catholique de Louvain;", "University College London"]},
{"conference": ["NSDI '12 Technical Sessions | USENIX"], "fulltext": ["https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final126.pdf"], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">In this paper, we observe that bandwidth sharing via TCP in commodity data center networks organized in multi-rooted tree topologies can lead to severe unfairness, which we term as the <em>TCP Outcast problem</em>, under many common traffic patterns. When many flows and a few flows arrive at two ports of a switch destined to one common output port, the small set of flows lose out on their throughput share significantly (almost by an order of magnitude sometimes). The Outcast problem occurs mainly in taildrop queues that commodity switches use. Using careful analysis, we discover that taildrop queues exhibit a phenomenon known as <em>port blackout</em>, where a series of packets from one port are dropped. Port blackout affects the fewer flows more significantly, as they lose more consecutive packets leading to TCP timeouts. In this paper, we show the existence of this TCP Outcast problem using a data center network testbed using real hardware under different scenarios. We then evaluate different solutions such as RED, SFQ, TCP pacing, and a new solution called <em>equal-length routing</em> to mitigate the Outcast problem.</p><p>\u00a0</p></div>"], "author": ["Pawan Prakash, Advait Dixit, Y. Charlie Hu, and Ramana Kompella, "], "title": ["The TCP Outcast Problem: Exposing Unfairness in Data Center Networks"], "affiliation": ["Purdue University"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>AOL Technologies has created a scalable object store for web applications. The goal of the object store was to eliminate the creation of a separate storage system for every application we produce while avoiding sending data to external storage services. AOL developers had been devoting a significant amount of time to creating backend storage systems to enable functionality in their applications. These storage systems were all very similar and many relied on difficult-to-scale technologies like network attached file systems. This paper describes our implementation\u00a0and the operating experience with the storage system. The paper also presents a feature roadmap and our release of an open source version.</p></div>"], "author": ["Daniel Pollack, "], "title": ["A Simple File Storage System for Web Applications"], "affiliation": ["AOL Inc."]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Dealing with disk failures has become an increasingly common task for system administrators in the face of high disk failure rates in large-scale data centers consisting of hundreds of thousands of disks. Thus, achieving fast recovery from disk failures in general and high online RAID-reconstruction performance in particular has become crucial. To address the problem, this paper proposes IDO (Intelligent Data Outsourcing ), a proactive and zone-based optimization, to significantly improve on-line RAID-reconstruction performance. IDO moves popular data zones that are proactively identified in the normal state to a surrogate set at the onset of reconstruction. Thus, IDO enables most, if not all, user I/O requests to be serviced by the surrogate set instead of the degraded set during reconstruction.</p>\r\n<p>Extensive trace-driven experiments on our lightweight prototype implementation of IDO demonstrate that, compared with the existing state-of-the-art reconstruction approaches WorkOut and VDF, IDO simultaneously speeds up the reconstruction time and the average user response time. Moreover, IDO can be extended to improving the performance of other background RAID support tasks, such as re-synchronization, RAID reshape and disk scrubbing.</p></div>"], "author": ["Suzhen Wu, ", " Hong Jiang and\u00a0Bo Mao, "], "title": ["IDO: Intelligent Data Outsourcing with Improved RAID Reconstruction Performance in Large-Scale Data Centers"], "affiliation": ["Xiamen University and ", ";", "University of Nebraska-Lincoln"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Diagnosing performance problems in large distributed systems can be daunting as the copious volume of monitoring information available can obscure the root-cause of the problem. Automated diagnosis tools help narrow down the possible root-causes\u2014however, these tools are not perfect thereby motivating the need for visualization tools that allow users to explore their data and gain insight on the root-cause. In this paper we describe Theia, a visualization tool that analyzes application-level logs in a Hadoop cluster, and generates visual signatures of each job's performance. These visual signatures provide compact representations of task durations, task status, and data consumption by jobs. We demonstrate the utility of Theia on real incidents experienced by users on a production Hadoop cluster.</p></div>"], "author": ["Elmer Garduno, Soila P. Kavulya, Jiaqi Tan, Rajeev Gandhi, and Priya Narasimhan, "], "title": ["Theia: Visual Signatures for Problem Diagnosis in Large Hadoop Clusters  "], "affiliation": ["Carnegie Mellon University"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>After an initial trial, Trinity College has deployed iPads to its 600 international students. Supporting these devices on the Trinity network has presented challenges relating to configuration management, wireless network capacity, and server load. </p>\r\n<p>We addressed the problem of configuration management with a web application written using the Django framework, which enables students to download and install a customized configuration profile to their iPads.\u00a0</p>\r\n<p>This paper describes the requirements and implementation of the configuration management solution, security issues, as well as the experiences and lessons learned from its use.</p></div>"], "author": ["Tim Bell, "], "title": ["Lessons in iOS Device Configuration Management"], "affiliation": ["Trinity College, University of Melbourne"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p class=\"p1\">System administrators increasingly use declarative,\u00a0object-oriented languages to configure their systems.\u00a0Extending such systems with automated analysis and\u00a0decision making is an area of active research. We introduce\u00a0ConfSolve, an object-oriented declarative configuration\u00a0language, in which logical constraints over a\u00a0system can be specified. Verification, impact analysis\u00a0or even the generation of valid configurations can then\u00a0be performed, by translation to a Constraint Satisfaction\u00a0Problem (CSP), which is solved with an off-the-shelf\u00a0solver. We present a full definition of our language and\u00a0its compilation process, and show that our implementation\u00a0outperforms previous work utilising an SMT solver,\u00a0while adding new features such as optimisation.</p></div>"], "author": ["John A. Hewson and\u00a0Paul\u00a0Anderson, ", " Andrew D.\u00a0Gordon, "], "title": ["A Declarative Approach to Automated Configuration"], "affiliation": ["University of Edinburgh;", "Microsoft Research and University of\u00a0Edinburgh"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Modern Web browsers do not provide sufficient protection to prevent users from submitting their online passwords to inappropriate websites. As a result, users may accidentally reveal their passwords for high-security websites to inappropriate low-security websites or even phishing websites. In this paper, we address this limitation of modern browsers by proposing LoginInspector, a profiling-based warning mechanism. The key idea of LoginInspector is to continuously monitor a user\u2019s login actions and securely store hashed domain-specific successful login information to an in-browser database. Later on, whenever the user attempts to log into a website that does not have the corresponding successful login record, LoginInspector will warn and enable the user to make an informed decision on whether to really send this login information to the website. LoginInspector can also report users\u2019 insecure password practices to system administrators so that targeted training and technical assistance can be provided to vulnerable users. We implemented LoginInspector as a Firefox browser extension and evaluated it on 30 popular legitimate websites, 30 sample phishing websites, and one new phishing scam discovered by M86 Security Labs. Our evaluation and analysis indicate that LoginInspector is a secure and useful mechanism that can be easily integrated into modern Web browsers to complement their existing protection mechanisms. Security system administrators in our university commented that such a tool could be very helpful for them to strengthen campus IT security.</p></div>"], "author": ["Chuan Yue,"], "title": ["Preventing the Revealing of Online Passwords to Inappropriate Websites with LoginInspector"], "affiliation": [" University of Colorado\u00a0at Colorado Springs"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Traditional Unix tools operate on sequences of characters, bytes, fields, lines, and files. However, modern practitioners often want to manipulate files in terms of a variety of language-specific constructs\u2014C functions, Cisco IOS interface blocks, and XML elements, to name a few. These language-specific structures quite often lie beyond the regular languages upon which Unix textprocessing tools can practically compute. In this paper, we propose eXtended Unix text-processing tools (xutools) and present implementations that enable practitioners to extract (xugrep ), count (xuwc ), and compare (xudiff ) texts in terms of language-specific structures. We motivate, design, and evaluate our tools around real-world use cases from network and system administrators, security consultants, and software engineers from a variety of domains including the power grid, healthcare, and education.</p></div>"], "author": ["Gabriel A. Weaver and Sean W. Smith, "], "title": ["XUTools: Unix Commands for Processing Next-Generation Structured Text"], "affiliation": ["Dartmouth\u00a0College"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>As system administrators who are also full-time students, we aim to minimize the time we spend approving and carrying out standard tasks that comprise much of our day-to-day work. The less time required for these repetitive tasks, the more time we have available to provide new and exciting services to our community. </p>\r\n<p>To facilitate the automation of this process, we have created the Grand Unified Task System (GUTS), which consists of a small core (a web interface and task executor) that unites task request processing for a range of modular services. This design allows for enhanced security and makes the system easily understandable and extensible, especially to new administrators. The Python backend provides deep integration with standard UNIX tools; the Django-based frontend provides a web interface friendly to both users and administrators. These design decisions have proven successful: deploying GUTS in production allowed us to dramatically reduce our response time for approving tasks, reach a much larger portion of our potential user base, and more easily support a diverse array of new services.</p></div>"], "author": ["Andrew Stromme, Dougal J. Sutherland, Alexander Burka, Benjamin Lipton, Nicholas Felt, Rebecca Roelofs, Daniel-Elia Feist-Alexandrov, Steve Dini, and Allen Welkie,\u00a0"], "title": ["Managing User Requests With the Grand Unified Task System (GUTS) "], "affiliation": ["Swarthmore College"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>With the advent of virtualization and cloud computing, virtualized systems can be found from small companies to service providers and big data centers. All of them use this technology because of the many benefits it has to offer, such as a greener ICT, cost reduction, improved profitability, uptime, flexibility in management, maintenance, disaster recovery, provisioning and more. The main reason for all of these benefits is server consolidation which can be even further improved through dynamic resource allocation techniques. Out of the resources to be allocated, memory is one of the most difficult and requires proper planning, good predictions and proactivity. Many attempts have been made to approach this problem, but most of them are using traditional statistical mathematical methods. In this paper, the application of discrete Bayesian networks is evaluated, to offer probabilistic predictions on system utilization with focus on memory. The tool Bayllocator is built to provide proactive dynamic memory allocation based on the Bayesian predictions, for a set of virtual machines running in a single hypervisor. The results show that Bayesian networks are capable of providing good predictions for system load with proper tuning, and increase performance and consolidation of a single hypervisor. The modularity of the tool gives a great freedom for experimentation and even results to deal with the reactivity of the system can be provided. A survey of the current state-of-the-art in dynamic memory allocation for virtual machines is included in order to provide an overview.</p></div>"], "author": ["Evangelos Tasoulas, ", "H\u00e2rek Haugerud, ", " Kyrre Begnum, "], "title": ["Bayllocator: A Proactive System to Predict Server Utilization and Dynamically Allocate Memory Resources Using Bayesian Networks and Ballooning "], "affiliation": ["University of Oslo; ", "Oslo\u00a0and Akershus University College;", "Norske Systemarkitekter AS"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>System administrators are often asked to apply their professional expertise in unusual situations, or under tight resource constraints. What happens, though, when the \u201csituation\u201d is a foreign country with only basic technical infrastructure, and the task is to bauild systems which are able to survive and grow in these over-constrained environments?</p>\r\n<p>In this paper we report on our experiences in two very different countries \u2013 Cuba and Ethiopia \u2013 where we ran a number of ICT projects. In those projects we assisted local universities to upgrade their ICT infrastructure and services. This included skills and process building for local system administrators.</p>\r\n<p>Based on our experiences we formulate a model for sustainable ICT capacity building. We hope this model will be useful for other organizations doing similar projects.</p></div>"], "author": ["Rudy Gevaert, "], "title": ["A Sustainable Model for ICT Capacity Building in Developing Countries   "], "affiliation": ["Ghent University, Belgium"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>For the past twelve years I have taught a one-term college-level class introducing students to the discipline of system administration. I discuss how the class was created, the considerations that went into designing the class structure and assignments, student outcomes, how the class has evolved over time, and other observations on teaching. Links to detailed course materials and other resources are provided.</p></div>"], "author": ["Steve VanDevender, "], "title": ["Teaching System Administration"], "affiliation": ["University of Oregon"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>This paper describes training and professional development activities at a mid-sized IT consulting firm over the last roughly 15 years. These activities have successfully engaged many of the consultants and provided significant career bonuses and advantages for the company. We present the types of activities, their effectiveness and success, and the evolution of professional development efforts over time. Many of these activities proved effective and valuable and are still in use, including annual skill reviews and development recommendations, regular organized training and discussion type events, training and materials reimbursements, and escalation support. Challenges and failures with other training and activities are described. Recommendations are made for other organizations\u2019 own professional development programs.</p></div>"], "author": ["George William Herbert, "], "title": ["Training and Professional Development in an IT Community"], "affiliation": ["Taos Mountain,\u00a0Inc."]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Monitoring is a core function of systems administration, and is primarily a problem of communication \u2013 a good monitoring tool communicates with users about problems, and communicates with hosts and software to take remedial action. The better it communicates, the greater the confidence administrators will have in its view of their environment. Nagios has been a leading open-source monitoring solution for over a decade, but in that time, the way it gets data in and out of its scheduling engine hasn\u2019t changed. As applications are written to extend Nagios, each one has to figure out its own way of getting data out of the Nagios core process. This paper explores the use of messaging middleware, in an open-source project called NagMQ, as a way to provide a common interface for Nagios that can be easily utilized by a variety of applications.</p></div>"], "author": ["Jonathan Reams, "], "title": ["Extensible Monitoring with Nagios and Messaging Middleware"], "affiliation": ["CUIT Systems Engineering,\u00a0Columbia University"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Today, network monitoring becomes necessary on many levels: Internet Service Providers, large companies as well as smaller entities. Since network monitoring supports many applications in various fields (security, service provisioning, etc), it may consider multiple sources of information such as network traffic, user activity, network events and logs, etc. All these ones produce voluminous amount of data which need to be stored, visualized and analyzed for administration purposes. Various techniques to cope with scalability have been proposed as for example sampling or aggregation.</p>\r\n<p>In this paper, we introduce an aggregation technique which is able to handle multiple kinds of dimension, i.e.  features, like traffic capture or host locations, without giving any preference a priori to a particular feature for ordering the aggregation process among dimensions. Furthermore, feature space granularity is determined on the fly depending on the desired events to monitor. We propose optimizations to keep the computational overhead low.</p>\r\n<p>In particular, the technique is applied to network related data involving multiple dimensions: source and destination IP addresses, services, geographical location of hosts, DNS names, etc. Thus, our approach is validated through multiple scenarios using different dimensions, measuring the impact of the aggregation process and the optimizations as well as by highlighting the ability to figure out important facts or changes in the network.</p></div>"], "author": ["Lautaro Dolberg, J\u00e9r\u00f4me Fran\u00e7ois, and Thomas\u00a0Engel, "], "title": ["Efficient Multidimensional Aggregation for Large Scale Monitoring  "], "affiliation": ["University of Luxembourg SnT\u2014Interdiciplinary\u00a0Centre for Security, Reliability and Trust"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>The automated identification of network service dependencies remains a challenging problem in the administration of large distributed systems. Advances in developing solutions for this problem have immediate and tangible benefits to operators in the field. When the dependencies of the services in a network are better-understood, planning for and responding to system failures becomes more efficient, minimizing downtime and managing resources more effectively.</p>\r\n<p>This paper introduces three novel techniques to assist in the automatic identification of network service dependencies through passively monitoring and analyzing network traffic, including a logarithm-based ranking scheme aimed at more accurate detection of network service dependencies with lower false positives, an inference technique for identifying the dependencies involving infrequently used network services, and an approach for automated discovery of clusters of network services configured for load balancing or backup purposes. This paper also presents the experimental evaluation of these techniques using real-world traffic collected from a production network. The experimental results demonstrate that these techniques advance the state of the art in automated detection and inference of network service dependencies.</p></div>"], "author": ["Barry Peddycord III and Peng Ning, ", " Sushil Jajodia, "], "title": ["On the Accurate Identification of Network Service Dependencies in Distributed Systems"], "affiliation": ["North Carolina State University;", "George Mason University"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>In this paper, the authors discuss their experiences implementing, operating, and optimizing a content delivery network for Canada\u2019s largest news website, that of the Canadian Broadcasting Corporation (CBC). The site receives over one million unique visitors per day. Although CBC uses the Akamai Aqua Platform (formerly EdgeSuite) as its content delivery network of choice, the lessons described here are generally applicable to any infrastructure fronted by a CDN.</p></div>"], "author": ["Julian Dunn, ", " Blake Crosby, "], "title": ["What Your CDN Won't Tell You: Optimizing a News Website for Speed and Stability"], "affiliation": ["SecondMarket Holdings, Inc.;", "Canadian Broadcasting Corporation"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>A look at the logging infrastructure that one division of Intuit built that included the requirement to handle 100K lines of logs per second with the logs being delivered to several destinations (including proprietary appliances).</p>\r\n<p>This paper will cover the options considered, the choices made and the problems we ran into. The most unusual and interesting topic discussed is the method selected to distribute the logs to all the different destinations, able to deliver the log message to several different load balanced farms of servers with only one copy being sent over the wire. </p></div>"], "author": ["David Lang, "], "title": ["Building a 100K log/sec Logging Infrastructure"], "affiliation": ["Intuit"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>In this paper we describe the design and implementation of a system essential to enable the deregulation of the energy market in the Netherlands. The system is used to test and validate secure communications using XML messages through the AS2 standard between the business partners in the market. The tool is comprised of an Enterprise Service Bus component, a service virtualization component, a database with business logic and an user interface added. The version 1.0 of the system was built in less than one month.</p></div>"], "author": ["Rudi van Drunen, ", " Rix Groenboom,\u00a0"], "title": ["Building a Protocol Validator for Business to Business Communications  "], "affiliation": [";", "Parasoft Netherlands"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>In 2008, the US Federal government mandated that all Federal-owned DNS zones must deploy DNSSEC. Initial deployments lagged and were often error prone. <br><br>This prompted the creation of a Tiger Team to assist agencies in deployment as well as a continuous monitoring program. These steps increased the number of signed zones in the .gov TLD and improved the response time in responding to errors and mistakes in deployment. This talk will cover the progress of DNSSEC in the Federal government in addition to lessons learned in setting up a system to monitor and maintain compliance across multiple administrative boundaries.</p></div>"], "author": [", ", "In 2008, the US Federal government mandated that all Federal-owned DNS zones must deploy DNSSEC. Initial deployments lagged and were often error prone."], "title": ["DNSSEC Deployment in .gov: Progress and Lessons Learned"], "affiliation": ["National Institute of Standards and Technology (NIST)"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>In this paper we describe the network infrastructure we designed, built and operated for the International Mathematics Olympiad in the Netherlands in 2011. The infrastructure was pragmatically designed around OpenVPN tunnels in a star topology between the various venues. VLANs were extensively used to separate functional groups and networks. The actual construction of the event network took about 3 days and was needed for only 2 weeks. The architectural, setup, building and operational aspects of the network are described and we include some lessons learned.</p></div>"], "author": ["Rudi van Drunen, ", "\u00a0Karst Koymans,\u00a0"], "title": ["Building the Network Infrastructure for the International Mathematics Olympiad  "], "affiliation": ["University of Amsterdam"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Faced with a fragmented research computing environment and growing needs for high performance computing resources, Michigan State University established the High Performance Computing Center in 2005 to serve as a central high performance computing resource for MSU\u2019s research community. Like greenfield industrial development, the center was unconstrained by existing infrastructure. The lessons learned are useful when building or maintaining an effective HPC resource and may provide insight for developing other computational services.</p></div>"], "author": ["Andrew R. Keen, Dr. William F. Punch, and Greg Mason,\u00a0"], "title": ["Lessons Learned When Building a Greenfield High Performance Computing Ecosystem"], "affiliation": ["Michigan State University"]},
{"conference": ["Technical Sessions | USENIX"], "fulltext": [], "description": ["<div class=\"field field-name-field-paper-description-long field-type-text-long field-label-hidden\"><p>Why do conference and school wireless networks always work so poorly? As IT professionals we are used to the network 'just working' and fixing things by changing configuration files. This mind-set, combined with obvious-but-wrong choices in laying out a wireless network frequently lead to a network that seems to work when it's tested, but that then becomes unusable when placed under load. This is at its worst at technical conferences where there are so many people, each carrying several devices, all trying to use the network at the same time, and in schools where you pack students close together and then try to have them all use their computers at the same time.</p>\r\n<p>Is this a fundamental limitation of wireless? While it is true that there are some issues that cannot be solved, there are a lot of things that the network administrator can do to make the network work better. The key issue is the obvious, but under-appreciated fact that wireless networking is radio communications first. If your radio link doesn't work well, you have no chance of fixing it with your configuration and software. This paper is intended to give you an appreciation of what the issues are, and enough information to know what sorts of things to look out for when planning a high density wireless network.</p></div>"], "author": ["David Lang, "], "title": ["Building a Wireless Network for a High Density of Users  "], "affiliation": ["Intuit"]}]